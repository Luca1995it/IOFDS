\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{float}

\usepackage{amsmath,amssymb}
\DeclareMathOperator{\improving}{IMPROVING-NEIGHBOR}
\DeclareMathOperator{\neighbor}{NEIGHBOR}
\DeclareMathOperator{\bestneighbor}{BEST-NEIGHBOR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}
 
\title{IOFDS Zusammenfassung}
\author{Luca \& Marty \thanks{Thanks to Rick and Morty}}
\date{June 2018}
 
\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}
 
\section{Greedy and Local Search}

Most of everyday's life problem are related to solving optimization problems, that is, to find a value $X$ that maximizes/minimizes a function $f(X)$, for example find a solution to the TSP problem. If $X$ is defined by a discrete set of possibilities, one speaks about discrete optimization. On the contrary, continuous optimization considers real-valued inputs. First, most real-world problem have to do with choices among a discrete set of alternatives, and solutions are often obtained with small steps.

\subsubsection{TSP}
The Traveling Salesman Problem is a simple but very hard to be solved problem. The problem can be shown to be NP-complete, so an exact solution is pretty impossible to be found in reasonable time for large instances. The number of possibile path is $(n-1)!$, so problem with more than 20-30 cities or nodes are not tractable. Luckily, good, but probably not optimal, solutions can be found with heuristic and optimization algorithms in reasonable time.

\subsection{Greedy}
\textit{``Better an egg today than a hen tomorrow''}\\
Greedy algorithms choose always the move that looks better at the moment. The base algorithm does not learn from the history and this shortsighted approach may prevent finding the best solution. There is no mechanism to undo moves, so every choice is definitive. Greedy can lead to very low quality solutions, but there are exceptions where a greedy approach is proved to lead to the best result, like with the MST problem.
A general way to prove optimality of greedy algorithms is as follows:
\begin{itemize}
\item{1. Formulate the problem in an inductive way}
\item{2. Prove that exists a solution starting with a greedy choice}
\item{3. Prove that $\text{optimal partial solution} + \text{greedy choice} \rightarrow \text{bigger partial optimal solution}$}
\end{itemize}

\subsection{Local Search and perturbations}
Improve a suboptimal solution with small local changes. We call this moves $\mu_{0}, \mu_{1}, ...$ and a move is kept only if it improves the actual solution. We call $X^{(0)}, X^{(1)}, ... , X^{(t)}$ the configurations of the input variable $X$ at step $t$. We define a simple function $N(X^{(t)}) = \{\mu_{i}(X^{(t)}), i = 0, ... ,n\}$ that returns the set of possible $n$ next moves from the current point $X^{(t)}$. The basic Local Search algorithm works as follow:
\begin{itemize}
\item{1. $X' \leftarrow \improving(N(X^{(t)}))$}
\item{2. $X^{(t+1)} = \begin{cases} 
      Y & f(Y) < f(X^{(t)}) \\
      X^{(t)} & \text{otherwise, and stop local search}.
   \end{cases}
$}
\end{itemize}
The solution is called local minimizer, and is the lowest value of the function in that valley. Local search has been discovered to be very effective on combinatory problems with a rich internal structure (like TSP). Local modifications of the tour are can be obtained removing some edges and reconnecting the nodes in a different manner, and the new $f(X)$ can be quickly computed with incremental evaluation.

\subsubsection{LS and Big valleys}
Local search is the initial building block of more complex schemes. We define an attraction basin as the portion of the input space that leads to the same local minima. The big valley property is a nice assumption that allow algorithms to perform better: it says that good local minima are often in good company, so their attraction basins are very close or neighbouring. This means that when local search stops in a local minima, it's often better try to escape from the current attraction basin and fall in a close one than restarting from a random point.




\section{Stochastic Global Optimization}
In this chapter techniques to find global opt for functions in real variables are covered. Brute force techniques (enumeration of all possible solutions) are impossible. 
A robust brute force technique can be throwing random points on the input space.

SGO is a set of methods which includes global random search and methods based on probabilistic assumption on the obj function. 

There is a problem: when num of variable increases, there is exponential increase in computational complexity, due to the fact that neighbours are exponentially further from each other.
 
Many algorithm exist, like simulated annealing.

\subsection{Stochastic Global Optimization Basics}
The optimization problem is:

given $ f : A \rightarrow R $ \\
find $ x\* \in A $ \\
such that $ f(x\*) \leq f(x) $ for every $ x \in A $ \\

$ x\ast $ is the global optimum.

It could be necessary to combine robust global schemes with fast local search schemes.

Multi-start local search: many runs of local search are run from a set of interesting points, so global scheme has to find an area of search, where a minimizer x* is found by local opt technique.

Strong assumption are needed to ensure convergence for sparse sequences of points. 

\subsubsection{Lipschitz continuity}
Function f does not have to change too fast, so it has a limited variability, or Lipschitz continuity. 

A function is L continuous if its change in value is bounded by the corresponding change in the evaluation point, multiplied by a constant factor $ K \geq 0 $.
$$
\mid f(x1)-f(x2) \mid < K \parallel x1 - x2 \parallel
$$ 

\subsection{Pure random search}
It is based only on repetitions of generated random points which have to be evaluated. They are generated usually with uniform distribution. 

Often used as component of more complex global optimization schemes. 

Convergence can be proved (soon or late, I reach the optimum using uniform dist), but rate of convergence is more interesting. Uniform probab distribution does not learn from previous history of search, so to speed up convergence we can combine a probab density Q with uniform prob distribution P, so that the convergence theorem still holds.

\subsubsection{Rate of convergence in PRS}
Different samples $ x_{j} $ are independent and identically distributed (i.i.d) with distribution p(x), and f has to hit the “target” set 
$$ 
B \big(x\ast , \epsilon \big) = \{ x \in A : \parallel x - x\ast \parallel \le \epsilon \}
$$

with point $ x_{j} $.

It can be a success or a failure, so it can be model with indipendent bernoully trials.

probab of failure is:
$$
Pr \{ x_j \notin B \} = 1-p(B)
$$

sample points are indipendent so I can multiply probab:

$$
(1-p(B))^n
$$

So average num of iterations required for a first hit of a ball B is:
$$
E(\text{first hitting time}) = 1 / p(B)
$$

Finally $ p(B) $ can be computed as the ratio between the vol of the ball B and the vol of the feasible reagion A:

$$ 
\frac{vol(A)}{vol(B)} = \frac{\pi^{d/2}*\epsilon^d}{\Gamma(d/2+1)*vol(A)}
$$

So now we can see that $ N\ast $ is:

$$
N\ast \approx -ln{\gamma} * \frac{\Gamma(d/2+1)}{\pi^{d/2}*\epsilon^{d}}*vol(A)
$$

Number of iteration strongly depends on the dimension \textit{d} of the search space. If \textit{d} increases, the number of iteration of PRS increase exponentially. 

The only thing we can hope is to deal with functions with regularities which can be learnt from an initial sampling, to identify shortcuts and get closer to the opt solution. 

Convergence does not mean anything in practice.

\subsection{Statistical inference in global random search}

The objective is to answer: what is the probability to get a new record value in the next n
iterations?  In some cases one needs a large number of sample points in the vicinity of the global optimizer in order to derive estimates.

Two application of stat inference are branch and bound and random multistart.

Branch and probab bound generalize branch and bound to continuous varibles.

Steps: 
\begin{itemize}
\item split set of admissible values in a tree-style :)
\item decide the potential value of individual subsets for other searches
\item select the most interesting subsets fo additional splits
\end{itemize}
	
Hyper-rectangles used for splitting space. Can be used for low-dim problems.

Random multistart repeats local search from random init points, until each search find a local min. We can use stat inf to estimate number of trials initial points to generate to guarantee that all loc min have been found. 

\subsection{Markov process and simulated annealing}

Local search can be modified in stochastic way. There are many ideas to do such a thing and they can be applied in continuous and discrete problems. 

Local search stops at a locally optimal point, searching in the vicinity of good local minima may lead to the
discovery of better solutions. I can accept worsening moves, hoping to reach in future better local min. The danger is to fall back again to the local min, because the next iteration cannot escape from the local basin. 

To avoid deterministic cycles I can use Simulated Annealing. I allow worsening moves but I still visit low \textit{f} values more often. I "shake" the function to visit worse point and then I try to find states with lower values than the initial one.

\subsection{Symulated Annealing and Asymptotics}
In PRS probab distribution for generating sample can depend on previously extracted points. A simplification is  to have the distribution just depend on the latest generated point and \textit{f} value. So, the sequence of sample points become a Markov chain, which has no memory on the history of previous search. This is not efficient.
SA is based on Markov but trajectory is built in ranomized manner: successor of current point is chosen stochastically, wiht probab depending only on the current point.

\begin{itemize}
\item{1. $x' \leftarrow \neighbor(N(X_j))$}
\item{2. $x_{(j+1)} = \begin{cases} 
      x' & \text{ if } f(x') \leq f(x_j) \\
      x' & \text{ if } f(x') > f(x_j) \text{ with probability } p = \exp{-\frac{f(x)-f(x_j)}{T}} \\
      x_j & \text{otherwise}
   \end{cases}
$}
\end{itemize}

SA introduces a temperature parameter T which determines the probability that worsening moves are accepted:
a larger T implies that more worsening moves tend to be accepted, and therefore a larger diversification occurs. The rule in the equation is called exponential acceptance rule.

If T is big (heating) , I obtain a random walk. If T is lowerd, I accept only improving moved (cooling).

\subsection{Asymptotic Convergence Results}

Many bad formulas to show that hen the number of iterations goes to infinity and the temperature is decreased slowly, the probability of
observing a global optimum goes to one but I have to wait toooooo loooong.

(in future there will be some tricks to use SA in online manner, learning the parameter T during search and adapting it in smart ways :) )

Actually, repeated local search, and even pure random search have better asymptotic results
for some problems.

\subsection{Inertial Shaker Algorithm}
The simpler Inertial Shaker (IS) technique, can be a practical choice to go beyond PRS while allowing on-the-job learning to rapidly adapt the probab of generating the next sample.

\begin{itemize}
\item probab is uniform over a search box defined around a current point x, where samples are generated. than, the search box is moved following the displacement applied to the point x
\item trend direction is computed averaging previous displacements
\item amplification and history depth are used to compute the displacement
\end{itemize}

Double shot strategy applied to all component of the vector of displacements. A displacement
is applied at every component as long as it improves the result. If no improvement is possible, then the function returns false, and the search box is accordingly shrunk.

\begin{figure}
\includegraphics[scale=0.60]{is}
\caption{inertial shaker alg}
\centering
\end{figure}


\section{Derivative Based Optimization}

Derivative based optimization techniques help solving problems on continuous variables. We will focus on two major related problems:
\begin{itemize}
\item{Solving a set of non-linear equations \\
	\begin{itemize}
	\item{ $ \text{Given } F : {\rm I\!R}^n \rightarrow {\rm I\!R}^n $ }
	\item{ $ \text{Find } x^\ast \in {\rm I\!R}^n \text{ such that } F(x^\ast) = 0 \longrightarrow \text{ if } x^\ast \text{ exists it minimizes } \sum_{i=1}^{n} f_i(x)^2 = 0 $ }
	\end{itemize}
}\bigskip
\item{
	Unconstrained minimization \\
	\begin{itemize}
	\item{ $ f : {\rm I\!R}^n \rightarrow {\rm I\!R}^n $ }
	\item{ $ \text{Find } x^\ast \in {\rm I\!R}^n \text{ such that } f(x^\ast) \leq f(x) \ \forall x \in {\rm I\!R}^n \longrightarrow x^\ast \text{ if global minima } $ }
	\end{itemize}
}
\end{itemize}
This chapter will collect some techniques to optimize smooth (derivable) functions on continuous variables. A function is derivable in a if exists the limit $$
f'(a) = \lim_{h\to0} \frac{f(a+h) - f(a)}{h} $$ If $x$ is very close to the point of interest $a$ ($h = x-a$), we can approximate the function in $x$ with $$ f(a+h) \approx f(a) + f'(a)*h $$ Like skis are tools for descending slopes, derivatives are useful tools to build local model and to minimize the objective function iteratively.

\subsection{Optimization and machine learning}

There is a deep relation between Optimization and ML because each one is used by the other to improve performances. ``Optimization for Learning'' is the art of choosing best model and variables configurations to let our machine learning system perform better. An example is to choose the best combination of weights in a NN. On the other side ``Learning for Optimization'' is the science of using learning techniques to improve the search of global minima that depends on the past steps done by the algorithm. This allows to build algorithms that have a memory of what they already did, for example to avoid visiting a non-interesting attraction basin more than once.

\subsection{Derivative based techniques in one dimension}
We start with a historical and fundamental method to find the point where the function is equal to 0, starting from a sufficiently close point and repeating the following steps:
\begin{itemize}
\item{Find a solvable local model}
\item{Solve the local model}
\end{itemize}
The local model can be found with:
\begin{itemize}
\item{the Taylor series approximation: $ f(x) = f(x_{c}) + f'(x_{c}) (x - x_{c}) + \frac{f''(x_{c}) (x-x_{c})^2}{2!} + ...$ }
\item{or the Newton's theorem: $ f(x) = f(x_{c}) + \int_{x_{c}}^{x} f'(z) dz \approx f(x_{c}) + f'(x_{c}) (x - x_{c}) $ }
\end{itemize}
The local model is therefore $$ M_{c}(x) = f(x_{c}) + f'(x_{c}) (x - x_{c}) $$ and so the next value $x_{+}$ will be $$ x_{+} = x_{c} - \frac{f(x_{c})}{f'(x_{c})} $$ 
If the function is linear, convergence occurs in one step, on the other case, it is possible to prove that if one starts close enough the the local minima, it will reach it in a fast (quadratic) manner.
\par
A global and safe algorithm to find roots of a function is the bisection method: if the function is continuous and we know at least two point $l_{0}$ and $r_{0}$ such that $f(l_{0}) < 0$ and $f(r_{0}) > 0$, the bisection method converges in a logarithmic number of steps. The cons is that it works only in one dimension.
\par
An other idea is to use backtracking: if Newton steps leads too far, one reverts the direction returning closer to the root. Finally, what usually expert do is to combine some powerful local convergence method (like Newton's) with others that are global and robust.
\par
Derivatives are not always available but can be approximated with secants. One can use the value of the previous iteration $f(x_{-})$ to find an approximation of the inclination: $$ a_{c} = \frac{f(x_{c}) - f(x_{-})}{x_{c} - x_{-}} $$ Given the less accuracy, convergence will be slower. 

\subsection{Solving models in more than one dimension}
Newton's method requires the gradient to be equal to zero. Given step $s$, the quadratic model is: $$Q(s) = \sum_{i=1}^{n} g_{i} s_{i} + \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij} s_{i} s_{j} \equiv g^{T} s + \frac{1}{2} s^{T} H s$$ The quadratic model is an reduction of the Taylor series to the second order, trying to approximate the objective function with a parabola in $n$ dimensions. Like many other times, we are searching a point where the gradient equals 0: $$ \nabla Q(s) = 0 = g +Hs $$ $$ Hs^{n} = -g \text{ (Newton equation)} $$ Because of the limited precision of real number on computers, one has to deal with numerical stability, so the computed value can be slightly different from the real solution. Ill-Conditioning is a term used to measure how much a function is sensitive to some errors in the data. There are cases in which very small oscillations in the input values lead to big changes in the output value of f, and has to be avoided.

\subsubsection{Gradient or steepest descent}
Often one has to work with many many dimensions and quadratic forms becomes inefficient. Moreover the Hessian H matrix is not always available and even an approximation in difficult to be computed. The base form of gradient descent updates the actual $x$ value in this way: $$ x_{+} = x_{c} - \epsilon \nabla f $$ This method has no global view of the actual situation, only local informations are available. A frequent problem arise with ill-conditioned matrices: when the quadratic form is ill-conditioned, the gradient inherits the computational errors and does not precisely point toward the local minima. So, it's important to design algorithms that do not ``zigzag'' toward the local minima but that try to keep a pretty linear trajectory, reducing so the time needed to reach the bottom of the valley.

\subsection{Non linear Optimization in more Dimensions}
The extension of the Newton's method in more dimensions consists in solving the following model: $$ m_{c} (x_{c} + p) = f(x_{c}) + \nabla f(x_{c})^T p + \frac{1}{2} p^T \nabla^2 f(x_{c}) p $$ Basically, given that we know the value and the gradient of the function in $x_{c}$, it is possible to approximate the value in close points $(x_{c} + p)$. If the initial point is close enough to the minimizer $x^*$ and the matrix $\nabla^2 f(x_{c})$ is positive definite, the method converges Q-quadratically to $x^*$. (If the Hessian H or it's approximation $\nabla^2 f(x_{c})$ are not positive definite, the method is not guaranteed to converge because the gradient is not guaranteed to point to the local minima).

\subsubsection{Global Convergence through Line Searches}
Global convergence is obtained by adopting line searches along the identified direction: one tries Newton's method first and then possibly backtracks.

\subsubsection{Secant methods}
Useful when Hessian H matrix is not available or too costly to be calculated. Given two near points, second derivative can be obtained calculating the ``variation of the gradient''. Like derivatives are given by the amount of change of the function in a given step $x_{2} - x_{1}$, second derivatives can be seen as the variation of first derivatives in $x_{2} - x_{1}$ (one-dimensional example):
$$ \frac{d^2 f(x)}{d x^2} (x_{2} - x_{1}) \approx \bigg( \frac{df(x_{2})}{dx} - \frac{df(x_{1})}{dx} \bigg) $$
In more dimensions one equation is not sufficient. Let the current and next point be $x_{c}$ and $x_{+}$, respectively, and let's define $s_{c} = x_{+} - x_{c}$ and $y_{c} = \nabla f(x_{+}) - \nabla f(x_{c})$ (difference of gradients). The analogous ``secant equation'' is
$$
H_{+} s_{c} = y_{c}
$$
Given that this equation has $O(n^2)$ admissible solution, past history is often used to choose the best one.

\subsubsection{Closing the GAP: second-order methods with linear complexity}
Computing the exact Hessian matrix (the one that contains the coefficients of the second order derivatives or ``the gradient of the gradient'') requires $O(n^2)$ time and $O(n^2)$ memory, in addition to the $O(n^3)$ complexity of finding next step in Newton method. OSS (One-step-secant) uses the info about the last gradient to approximate the new H matrix in linear time $O(n)$ with the secant method.
The new search direction $p_{+}$ is computed as follow:
$$
p_{+} = -g_{c} + A_{c} s_{c} + B_{c} y_{c}
$$
with $A_{c}$ and $B_{c}$ that weights first and second derivatives.

\subsection{Constrained Optimization and Lagrange Multipliers}
This section will shows briefly hot to deal with contraints, that is, with some restrictions on the input (features) space. A general constrained minimization problem is: 
$$
\min_{x}		f(x)$$
$$\text{subject to }		(g_{i} (x) = c_{i}) \ \forall i = 1,...,n \ \ \text{(Equality Contraints)}$$
$$			(h_{j} \geq d_{j}) \ \forall j = 1,...,m \ \ \text{(Inequality Contraints)}$$

Contraints are divided in two categories:
\begin{itemize}
\item{Hard Contraints, that must be respected at all cost (very rare in practice)}
\item{Soft Contraints, that can be violated paying a penality}
\end{itemize}
Soft Constraints are more interesting; an example of penalized objective function is:
$$
\min_{x} f(x) + \sum_{i} \gamma_{i} (g_{i}(x) - c_{i})^2
$$
In this case, the penalties increase quadratically with the ``error'' $(g_{i}(x) - c_{i} )$. If the function is smooth (derivable) and has partial derivates, this type of problem are usually solved with Lagrange multipliers:

$$
\min_{x} f(x) + \sum_{i} \lambda_{i} g_{i}(x)
$$ 

Minimizing the transformed function yields a necessary condition for optimality. Additional checks are therefore necessary (for example, the identified point can be a saddle point and not a global minimum), but in many cases, in the presence of a single global optimum, the method of Lagrange multipliers will deliver the correct solution.



\section{Reactive Search Optimization (RSO): Online Learning methods}

The optimization function is considered as a black-box, but the more points are generated the more knowledge is accumulated in an implicit form. This data can be used to generate internal models and improve efficiency of our algorithms. RSO tends towards truly intelligent problem-solving machines, which learn and self-improve the more they work.
RSO combines machine learning techniques with optimization heuristics. Reactive means ready response to events during the search, through an internal feedback loop for online self-tuning and dynamic adaptation. Lot of machine learning methods like reinforcement learning and NN are used, trying for example to model the probability density function the target function.

\subsection{RSO: Learning while searching}
The main objective is to tune parameters automatically, like the temperature in SA. The intent is to transfer part of the human intelligence to the machine allowing the algorithm to work independently. It is very important that the following objectives are reached:
\begin{itemize}
\item{Reproducibility of results: given a complete and unambiguous documentation, everyone should achieve the same performance}
\item{Automation: tuning of the parameters should be automatic}
\end{itemize}
Given that real-worlds problem have a rich internal structure, this algorithm should learn on the job like the human brain, deriving future decision based on previous observations. Knowing why and when an algorithm is effective is a very valuable property. 

\subsection{RSO based on prohibitions}
The idea is to prohibit old solutions to be more creative and to reach unexplored new area. A we already mentioned, local search generates a trajectory $X^{(t)}$ of points in the admissible search space. The successor of a point $X^{(t)}$ is selected from a Neighborhood $N(X^{(t)})$. A point is locally optimal with respect to $N$ if $f(X^{(t)}) < f(Y) \ \forall \ Y \in N(X^{(t)})$. In this chapter we will consider a case in which $X$ is a binary string on length $L: X = \{0,1\}^L$ and the neighbourhood is obtained by applying elementary moves $\mu_{i}$ with $i = 1,...,L$ that change the \textit{i}-th bit of the string $X = [x_{1},...,x_{i},...,x_{L}]$. The algorithm work as follow: at each iteration $t$, we have:
\begin{itemize}
\item{The set of all possible moves $M^{(t)}$}
\item{The set of prohibited moves $T^{(t)} \in M^{(t)}$}
\item{The set of admissible moves $A^{(t)} \in M^{(t)}$}
\end{itemize}
The next point is given by:
$$
X^{(t+1)} = \mu^{(t)}(X^{(t)}) \text{ where } \mu^{(t)} = \argmin_{v \in A^{(t)}} f(v(X^{(t)}))
$$
Basically, the admissible move that leads to the best point is chosen. But how do we populate the set of prohibited moves $T^{(t)}$? It depends on the case. For example, to avoid cycles, one can prohibit a move for a period $\tau$ after having executed it. If $\tau$ time has passed from the last time move $\mu_{i}$ was executed, then it will be newly available. Prohibit moves for a finite amount of time will be fundamental in later phases to reach the global minima. In the string case, this means that after having flipped a bit we should wait at least $\tau$ steps to flip it again. $\tau$ should be learnied in a way that there will be the possibility to reach a new attraction basin in $\tau + 1$ steps. Prohibitions methods are effective on combinatorial optimization and sub-symbolic machine learning.

\subsection{Fast data structures for using the search history}
Access past events of the trajectory $X^{(0)}, X^{(1)}, ..., X^{(t)}$ is mainly done through hashing functions and radix-tree techniques. A first technique combines hashing with linked lists: each bucket of the hash-table points toward a linked list of all elements in that bucket. Each item can be retrieved in approximated constant time if the number of buckets if sufficiently large and the hash functions distributes value with a pretty uniform probability.

An other method consists in combining hashing with persistent red-black trees. This data structure will not be ephemeral (previous version will not be destroyed when inserting / deleting data) but will be partially persistent. RB trees are used to do insert/delete of elements in $O(\log n)$ and can return all their content in $O(n)$. Each RB tree stores a copy of the system at step $t$ and all trees are reachable through a hash-table. If more configurations $X^{(t)}$ have the same hash, they are stored in a linked list. Each element of the list will point to a different RB. This method allows us to:
\begin{itemize}
\item{Store all the trajectory of our algorithm}
\item{Retrieve each config in $O(1) \text{ (time for the hash) } + O(n) \text{ (time to a DFS of the tree) }$ }
\item{Add a new configuration incrementally, trying to use previously computed subtrees}
\item{Share subtrees between different RB trees to be efficient}
\end{itemize}

\section{Adapting Neighbourhoods and Selection}
Possible moves are collected into a set of different
neighborhoods (NH). During local search I have to select a 
NH and how to select one neighbor to be the next poin in the search trajectory.

The first is the most problematic issue, but we can use online learning strategies to solve both of them.

In first case I can decide NH a priori and selecting the neighbor is dynamic. The longer I evaluate the neighborhood, the better the chance of
identifying a move with a large improvement, but the shorter the total number of moves which one can execute in a slot of CPU time.

\subsection{VNS}
Fixing the NH a priori is not a good choice, I have to adapt it to the local config around the current point. 

I consider a set of NG, defined a priori and I want to use the most appropriate one during search. 

So VNS has to:

\begin{itemize}
\item understand which NH use and how many
\item how to schedule them (order?)
\item how to select neighbor inside the NH (best one? the first one found?)
\end{itemize}

The first issue can be decided based on detailed problem knowledge and preliminary experimentation.

The second issue can be solved in different manner. I can cycle randomly among the NH (no online training), or I can change NH when I reach a local min inside the current NH. 
As soon as a local minimum is reached for a specific NH, improving moves can be found in other NH. Online training is used because I have to change NH using the minimal diversification I need.

The third issue can be addressed in two ways: best improvement local search and first-move. The first method considers all neighbors (brute-force) and evaluate them, then I move to the one with the best value. The second one considers only a sample of possible moves, a subset of neighbors, so, finally I return the first candidate with better \textit{f} value. This method is adaptive because the exact number of neighbors evaluated before deciding the next move depends on the local properties in the config space around the current point.
I evaluate a small number of candidates in the early phase of the search, and identifying
an improving move will become more difficult during the later phases, close to local optimality.

As soon as search is successful, back to default NH.

VN Descent uses the defaul NH first and the other only if it fails (current point is local min for it). I can order NH according to the strength of perturbation.
Reduced VNS generate a random neighbor before moving or not, so in the selection of neighbor alg I find this line:

\begin{figure}[H]
\includegraphics[scale=0.60]{random}
\centering
\end{figure}

Skewed VNS moves also in worsening zones if the lead to search trajectory sufficiently far from current point (I need also distance function to control diversification and a parameter to regulate tradeoff movement distance/acceptance worse moves).

\begin{figure}[H]
\includegraphics[scale=0.50]{vns}
\caption{VND and skewed version}
\centering
\end{figure}

\section{Iterated Local Search}
We assume that an LS algorithm is available as a black-box. The first and simpler method consists in starting LS from a set of point uniformly distributed over the input space $X$. As usually, the problem with this method is that it does not learn while searching a local or global minima. The right way is to use history to produce better starting point, taking in account the assumption that local minima are usually clustered. Moreover, many problems allow to evaluate $f$ incrementally from close input values.
Incremental local search works as follow:
\begin{itemize}
\item{Perturb current local minima}
\item{Start local search from the modified solution}
\end{itemize}
As you may have noticed, it is similar to VNS. This simple but effective method is better than restarting always from a casual new point because from the law of large numbers, $f(X^{(t)})$ values tends to concentrate local minima around an average $\E[f(x^*)]$ after lot of trials. Starting from a new point close to the last local minima destroys the independency of the events, trying to skip the law of large numbers.

A more robust technique works as follow:
\begin{itemize}
\item{Find some local minima (e.g. launching local search from some random point)}
\item{Restart the search from this local minima in their neighbourhood}
\end{itemize}
This approach creates a hierarchy of nested local searches. Usually, the level of nesting is two (like in the example). An important question is: how do we find neighbour attraction basins? Create a perfect map of attraction basins is very difficult for obvious reasons. A simple method to reach a neighbour attraction basin consists in using perturbations: start with a weak perturbation and increase the power until a new attraction basin is found. To avoid cycles, use some randomness in the generation of perturbations. Strength history should be used to speed up the process of finding how strong the new perturbation should be.
Picture \ref{fig:ils} shows how IteratedLocalSearch works.

\begin{figure}[H]
\includegraphics[scale=0.25]{ils}
\caption{Iterated Local Search}
\centering
\label{fig:ils}
\end{figure}

\section{Adaptive Random Search}
This methods apply when partial or complete derivates are not available. Adaptive Random Search is based only on the knowledge of the visited points. The base version works as follow:
\begin{itemize}
\item{1. Choose initial point and initial search region;}
\item{2. Generate new candidate point $x_+$ with a certain probability distribution on the search region;}
\item{3. Search region is expanded if $f(x_+) < f(x_c)$, otherwise is compressed ($x_c$ previous point);}
\item{4. If $f(x_+) < f(x_c)$ then $x_+$ is new current point and search region is moved around it;}
\end{itemize}
For effective implementations, simple search region around the current point suffice. An example is using boxes: 
$$
\delta = \sum_{j} Rand(-1,1) * b_j
$$
 with $b_j$ as base vector. Note that this search region is not a sphere but a hyper-rectangle.

\subsection{RAS: adaptation of the sampling region}
RAS stands for Reactive affine shaker is a method to modify the search region in an online manner. RAS use an affine transformation to 
create the new search region. An affine transformation is the composition of a linear one followed by a translation (e.g. rotation, scaling, shifting, shear, ...) :
$$
x \rightarrow Ax + b
$$
An affine transformation has the following good properties:
\begin{itemize}
\item{Points which lie on a line continue to be collinear after the transformation}
\item{Ratios of distances along a line are preserved}
\end{itemize}

This algorithm tries to minimize the number of steps needed to reach the local minima. By compressing the search region only if next point is worse, this algorithm tries to do always the largest possible movement. A pseudocode example is shown in picture \ref{fig:ras}. $\Delta$ is a vector generated with a uniform probability distribution over the search region. The algorithm uses a double-shot strategy: if adding $\Delta$ does not lead to a better point, it will try to go in the opposite direction. Using a double-shot strategy heavy reduce the probability of generating sequences of unsuccessful events. At each iteration a matrix P is computer and each base vector $b_j$ is updated.
If the function found a new better point, P will increase the length of base vectors, otherwise it will compress the search region. $\rho > 1$ is the expansion ratio. 

\begin{figure}[H]
\includegraphics[scale=0.25]{ras}
\caption{Reactive Affine Shaker}
\centering
\label{fig:ras}
\end{figure}

\subsection{Repetitions for robustness and diversification}

In general, estimate the number of steps required for identifying a global minimum is impossible. RAS can be stopped for example when search region become smaller that a certain threshold. To be more robust, one can launch different instances of RAS starting from different random points, creating a sort of population of independent RAS searchers. More complex methods to coordinate them are showed in Chap 15.




\section{Satisfiability}
The satisfiability problem is one of the most important in computer science. It's a well known NP-Complete problem and consists in finding an assignment to all the variables of the boolean formula such that the whole formula is True. This problems are really simple to be understood and really difficult to be solved.

\subsection{Satisfiability and maximum satisfiability: definitions}
One is given a boolean formula in CNF, a conjunction of clauses. The problem consists in finding an assignment that satisfies the max number of clauses. If a formula has $n$ variables and $m$ clauses, it can be written as:
$$
\bigwedge\limits_{1 \leq i \leq m} ( \bigvee\limits_{1 \leq k \leq |C_{i}|} l_{ik})
$$
The set of clauses is called $\boldsymbol{C}$. If one assigns a weight $w_i$ to each clause, the problem because MAX W-SAT. If each clause contains a fixed number k of literals, the problem becomes MAX-k-SAT.

\subsubsection{Notation and graphical representation}
MAX-SAT is easy to define and excellent to visualize. Given a formula $ (u_1 \lor \lnot u_3 \lor u_5) \land (\lnot u_2 \lor \lnot u_4 \lor \lnot u_5 ) \land ( \lnot u_1 \lor u_3 \lor \lnot u_4 )$ the respective table is showed in Picture \ref{fig:bool}.
Truth values to variables are assigned by placing a black triangle to the left if the variable is true, to the right if it is false. Each literal is depicted with a small circle, placed to the left if the corresponding variable is true, to the right in the other case. If a literal is matched by the current assignment (e.g., if the literal asks for a true value and the variable is set to true, or if is asks for false and the variable is false), it is shown with a gray shade. The coverage of a clause is the number of literals in the clause that are matched by the current assignment, and it is illustrated by placing a black square in the appropriate position of an array with indices ranging from 0 to the number of literals in each clause $|C|$.

\begin{figure}[H]
\includegraphics[scale=0.5]{bool}
\caption{Table of a boolean formula}
\centering
\label{fig:bool}
\end{figure}

\subsection{Resolution and Linear Programming}
\subsubsection{Resolution and backtracking for SAT}
Inspired by Branch and Bound adapting it to the structure of SAT. A basic tool we will need is that of resolution, given by the recursive replacement of a formula by one or more formulae, the solution of which implies the solution of the original formula. In resolution a variable is selected and a new clause, called the resolvent is added to the original formula. The process is repeated to exhaustion or until an empty clause is generated. The original formula is not satisfiable if and only if an empty clause is generated. 

A clause R is the resolvent of clauses $C_1$ and $C_2$ iff there is a literal $l \in C_1$ with $\lnot l \in C_2$ such that $R = (C_1 \setminus \{l\}) \cup (C_2 \setminus \{l\})$ and $u(l)$, the variable associated to the literal, is the only variable appearing both positively and negatively. For the two clauses $C1 = (l \lor a_1 \lor ... \lor a_A)$ and $C2 = (\lnot l \lor b_1 \lor ... \lor b_B)$ the resolvent is therefore the clause $R = (a_1 \lor ... \lor a_A \lor b_1 \lor ... \lor b_B )$. Is we add the resolvent as a new clause, the solution does not change, because if both $C_1$ and $C_2$ are satisfied, they have at least one literal in common that will solve the resolvent. If there is not common literal between $C_1$ and $C_2$, then they contain only $l$ and $\lnot l$ and so are not both satisfiable. Picture \ref{fig:resolvent} shows how to construct a resolvent.

\begin{figure}[H]
\includegraphics[scale=0.5]{resolvent}
\caption{How to construct a resolvent}
\centering
\label{fig:resolvent}
\end{figure}

One of the best SAT solvers is the DPLL algorithm. It's pseudocode is showed in Picture \ref{fig:dpll}.

\begin{figure}[H]
\includegraphics[scale=0.5]{dpll}
\caption{DPLL algorithm}
\centering
\label{fig:dpll}
\end{figure}

with as input a set of clauses $\boldsymbol{C}$. Remember that:
\begin{itemize}
\item{Pure literal: If a propositional variable occurs with only one polarity in the formula, it is called pure. Pure literals can always be assigned in a way that makes all clauses containing them true. Thus, these clauses do not constrain the search anymore and can be deleted.}
\item{Unit clause: If a clause is a unit clause, i.e. it contains only a single unassigned literal, this clause can only be satisfied by assigning the necessary value to make this literal true. Thus, no choice is necessary. In practice, this often leads to deterministic cascades of units, thus avoiding a large part of the naive search space.}
\item{$\boldsymbol{C}(l)$ means replacing the LITERAL $l$ with True in all clauses. If the literal is a negated variable, then it will be replaced with False.}
\end{itemize}

Unfortunately, in the worst case, this algorithm will require an exponential number of steps to find out if the formula is satisfiable. 

\subsubsection{Integer programming approaches}
The MAX W-SAT problem has a natural integer linear programming formulation. Let $y_j = 1$ if boolean var $u_j$ is true otherwise $0$. Let $z_i = 1$ if clause $C_i$ is satisfied else $0$. The integer linear programming problem is:
$$
\max \sum_{i=1}^{m} w_i z_i
$$
subject to the following constraints (it says that each clause should have at least a true assignment to a literal):
$$
\sum_{j \in U_i^{+}} y_j + \sum_{j \in U_i^{-}} (1 - y_j) \geq z_i , \ \ i = 1,...,m
$$
with $U_i^{+}$ and $U_i^{-}$ as the sets of positive and negative variables in clause $C_i$. With this formulation, a simple Branch and Bound method should be able to solve the problem and maximize it. Like in DPLL, from the root two branches are generated, one with a true assignment to a chosen variable and the other with a false assignment (last two lines of the algorithm).

\subsection{Continuous approaches}
Other methods to solve ILP is to solve a related problem of inductive inference, in which one aims at identifying a hidden boolean function using outputs obtained by applying a limited number of random inputs to the hidden function. The task is formulated as a SAT problem, which is in turn formulated as an ILP:
$$
A^T y \leq c, \ \ y \in \{-1,1\}^n
$$
where $A^T$ is an $m \times n$ real matrix and $c$ a real vector of length $m$. The so called interior point is based on finding the local minimum in the box $-1 \leq y_j \leq 1$ of a potential function $\phi(y)$. If there is a solution, it will be the solution of the ILP problem too.

\subsection{Approximation algorithms}
MAX-SAT is a playground for algorithms with guaranteed quality of approximation. The basic principle is to guarantee that the delivered result will be within a certain percentage of the optimal solution. The first approximate algorithm for MAX W-SAT were proposed by Johnson and is showed in Picture \ref{fig:johnson}.

\begin{figure}[H]
\includegraphics[scale=0.5]{johnson}
\caption{Johnson algorithm}
\centering
\label{fig:johnson}
\end{figure}

The Johnson algorithm can be showed to be $\frac{k}{k+1}$-approximate for MAX-SAT. An improved version, that is Johnson2, remembering that $k$ is the minimum number of literals in a clause. Picture \ref{fig:johnson2} shows the second version, that is $(1 - \frac{1}{2^k})$-approximate algorithm. 

\subsubsection{Randomized algorithms for MAX W-SAT}
A randomized $\frac{1}{2}$-approximate algorithm for MAX W-SAT. One of the most interesting approaches in the design of new algorithms is the use of randomization. During the computation, random bits are generated and used to influence the algorithm process. First see the algorithm \textit{RANDOM} in Picture \ref{fig:randomsat}.

\begin{figure}[H]
\includegraphics[scale=0.3]{randomsat}
\caption{The Random algorithm}
\centering
\label{fig:randomsat}
\end{figure}

The \textit{RANDOM} algorithm is $(1 - \frac{1}{2^k})$-approximate, and with a $k$ that can potentially be 1, we say it is $\frac{1}{2}$-approximate. It is easy to proof that if a clause has k literals, satisfiying at least one of them has probability  $(1 - \frac{1}{2^k})$.  An improved version, that differers only in the probability of setting variables to true, is shown in Picture \ref{fig:genrandom}.

\begin{figure}[H]
\includegraphics[scale=0.3]{genrandom}
\caption{The GenRandom algorithm}
\centering
\label{fig:genrandom}
\end{figure}

\textit{GenRandom} can be shown to be $\frac{3}{4}$-approximate for MAX W-SAT.

\subsection{Local Search for SAT}
MAX-SAT is among the problems for which perturbative local search is very effective: the general base scheme is based on generating a starting point in the set of admissible solution and trying to improve it through the application of simple basic moves. If a move is successful, current point is moved to the new location, otherwise on keeps the current point. Of course the successfulness of a local search technique depends on the neighbourhood chosen. In MAX-SAT, the input space will be the one given by all possible truth assignment and representing it as a binary string. The output will be the number of satisfied clauses. Formally, let $\Omega$ be the discrete search space: $\Omega = \{0,1\}^n$, and let $f:\Omega \rightarrow  {\rm I\!R}$ the function to be maximized. Then let $U^{(t)} \in \Omega$ be the current configuration along the search trajectory at step $t$ and $N(U^{(t)}) \subseteq \Omega$ the heighbourhood obtained applying a set of basic moves $\mu_i \ (1 \leq i \leq n)$ where $\mu_i$ complements the \textit{i}-th bit of the string: $\mu_i (u_1, u_2, ... , u_i, ... ,u_n) = (u_1, u_2, ..., \lnot u_i, ..., u_n)$.
$$
N(U^{(t)}) = \{U \in \Omega \text{ such that } U = \mu_i(U^{(t)}), \ i = 1,...,n\}
$$
The version of local search that we consider starts from a random initial configuration $U^{(0)} \in \Omega$ and generated a search trajectory as follows: 
\begin{itemize}
\item{1. $V \leftarrow \bestneighbor(N(U^{(t)}))$}
\item{2. $U^{(t+1)} = \begin{cases} 
      V & f(V) < f(U^{(t)}) \\
      U^{(t)} & f(V) \geq f(U^{(t)})
   \end{cases}
$}
\end{itemize}
Obviously, this algorithm stops when no other improving points can be reached.

An intriguing result by Koutsoupias and Papadimitriou shows that, for the vast majority of satisfiable 3-SAT formulae, the local search heuristic that start at a random truth assignments and repeatedly flips a variable that improves the number of satisfied clauses, almost always succeeds in discovering a satisfying truth assignment.

\subsubsection{Randomized search for 2-SAT (Markov process)}
While it is well known that 2-SAT is not NP-Complete, the MARKOV-SEARCH algorithm is interesting for its simplicity and its capacity to find a solution in $O(n^2)$ steps. it works as follow:
\begin{itemize}
\item{1. Start with a random truth assignment}
\item{2. While there are unsatisfied clause, pick one of them and flip a random literal in it}
\end{itemize}

Note that it accepts worsening moves.

\subsection{Memory-less Local Search Heuristics}
State-of-the-art heuristics for MAX-SAT are obtained by complementing local search with schemes that are capable of producing better approximations beyond the locally optimal points.
\subsubsection{SA for MAX-SAT}
The algorithm works like the normal SA, with MAX-TEMP and MIN-TEMP as bounds to the temperature. Figure \ref{fig:sasat} shows how it works.

\begin{figure}[H]
\includegraphics[scale=0.3]{sasat}
\caption{The SA for SAT}
\centering
\label{fig:sasat}
\end{figure}

\subsubsection{GSAT with ``random noise'' strategies}
SAT is of special concern to Artificial Intelligence because of its connection with reasoning. GSAT consists on multiple runs on LS, each run consisting in a number of iterations proportional to the input space dimension. GSAT-WITH-WALK use ``noise'' to escape from attraction basins, balancing random moves with deterministic ones. Picture \ref{fig:gsat} shows how it works:

\begin{figure}[H]
\includegraphics[scale=0.3]{gsat}
\caption{GSAT-WITH-WALK algorithm}
\centering
\label{fig:gsat}
\end{figure}


\subsection{History-sensitive Heuristics}

Different history-sensitive heuristics have been proposed to continue local search schemes beyond local optimality. These schemes aim at intensifying the search in promising regions and at diversifying the search into uncharted terri- tories by using the information collected from the previous phase (the history) of the search.

\subsubsection{Prohibition-based Search}
Prohibit some moves for a finite amount of time is a good practice to avoid local search fall in the same attraction basin many times and to avoid the creation of cycles. 

\subsection{Models of hardness and threshold effects}
How to benchmark our SAT solver or our MAX-SAT solver? There is the need to generate some test cases on which run our algorithms. There are two main models to generate MAX-SAT instances:
\begin{itemize}
\item{k-SAT modes, also called fixed length clause mode. A randomly generated CNF formula consists of independently generated random clauses, where each clause contains exactly $k$ literals. Each literal is chosen uniformly from $U = \{u_1 , ..., u_n \}$, and negated with probability $p$. The default value for $p$ is 1/2.}
\item{average k-SAT model, also called random clause model. A randomly generated CNF formula consists of independently generated random clauses. Each literal has ha probability $p$ of being part of a clause. In detail, each of the n variables occurs positively with probability $p(1 -p)$, negatively with probability $p(1 - p)$, both positively and negatively with probability $p^2$, and is absent with probability $(1 - p)^2.$}
\end{itemize}




\end{document}




