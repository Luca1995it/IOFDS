\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{float}

\usepackage{amsmath,amssymb}
\DeclareMathOperator{\improving}{IMPROVING-NEIGHBOR}
\DeclareMathOperator{\neighbor}{NEIGHBOR}
\DeclareMathOperator{\bestneighbor}{BEST-NEIGHBOR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathbb{E}}
 
\title{IOFDS Zusammenfassung}
\author{Luca \& Marty \thanks{Thanks to Rick and Morty}}
\date{June 2018}
 
\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}
 
\section{Greedy and Local Search}

Most of everyday's life problem are related to solving optimization problems, that is, to find a value $X$ that maximizes/minimizes a function $f(X)$, for example find a solution to the TSP problem. If $X$ is defined by a discrete set of possibilities, one speaks about discrete optimization. On the contrary, continuous optimization considers real-valued inputs. First, most real-world problem have to do with choices among a discrete set of alternatives, and solutions are often obtained with small steps.

\subsubsection{TSP}
The Traveling Salesman Problem is a simple but very hard to be solved problem. The problem can be shown to be NP-complete, so an exact solution is pretty impossible to be found in reasonable time for large instances. The number of possibile path is $(n-1)!$, so problem with more than 20-30 cities or nodes are not tractable. Luckily, good, but probably not optimal, solutions can be found with heuristic and optimization algorithms in reasonable time.

\subsection{Greedy}
\textit{``Better an egg today than a hen tomorrow''}\\
Greedy algorithms choose always the move that looks better at the moment. The base algorithm does not learn from the history and this shortsighted approach may prevent finding the best solution. There is no mechanism to undo moves, so every choice is definitive. Greedy can lead to very low quality solutions, but there are exceptions where a greedy approach is proved to lead to the best result, like with the MST problem.
A general way to prove optimality of greedy algorithms is as follows:
\begin{itemize}
\item{1. Formulate the problem in an inductive way}
\item{2. Prove that exists a solution starting with a greedy choice}
\item{3. Prove that $\text{optimal partial solution} + \text{greedy choice} \rightarrow \text{bigger partial optimal solution}$}
\end{itemize}

\subsection{Local Search and perturbations}
Improve a suboptimal solution with small local changes. We call this moves $\mu_{0}, \mu_{1}, ...$ and a move is kept only if it improves the actual solution. We call $X^{(0)}, X^{(1)}, ... , X^{(t)}$ the configurations of the input variable $X$ at step $t$. We define a simple function $N(X^{(t)}) = \{\mu_{i}(X^{(t)}), i = 0, ... ,n\}$ that returns the set of possible $n$ next moves from the current point $X^{(t)}$. The basic Local Search algorithm works as follow:
\begin{itemize}
\item{1. $X' \leftarrow \improving(N(X^{(t)}))$}
\item{2. $X^{(t+1)} = \begin{cases} 
      Y & f(Y) < f(X^{(t)}) \\
      X^{(t)} & \text{otherwise, and stop local search}.
   \end{cases}
$}
\end{itemize}
The solution is called local minimizer, and is the lowest value of the function in that valley. Local search has been discovered to be very effective on combinatory problems with a rich internal structure (like TSP). Local modifications of the tour are can be obtained removing some edges and reconnecting the nodes in a different manner, and the new $f(X)$ can be quickly computed with incremental evaluation.

\subsubsection{LS and Big valleys}
Local search is the initial building block of more complex schemes. We define an attraction basin as the portion of the input space that leads to the same local minima. The big valley property is a nice assumption that allow algorithms to perform better: it says that good local minima are often in good company, so their attraction basins are very close or neighbouring. This means that when local search stops in a local minima, it's often better try to escape from the current attraction basin and fall in a close one than restarting from a random point.




\section{Stochastic Global Optimization}
In this chapter techniques to find global opt for functions in real variables are covered. Brute force techniques (enumeration of all possible solutions) are impossible. 
A robust brute force technique can be throwing random points on the input space.

SGO is a set of methods which includes global random search and methods based on probabilistic assumption on the obj function. 

There is a problem: when num of variable increases, there is exponential increase in computational complexity, due to the fact that neighbours are exponentially further from each other.
 
Many algorithm exist, like simulated annealing.

\subsection{Stochastic Global Optimization Basics}
The optimization problem is:

given $ f : A \rightarrow R $ \\
find $ x\* \in A $ \\
such that $ f(x\*) \leq f(x) $ for every $ x \in A $ \\

$ x\ast $ is the global optimum.

It could be necessary to combine robust global schemes with fast local search schemes.

Multi-start local search: many runs of local search are run from a set of interesting points, so global scheme has to find an area of search, where a minimizer x* is found by local opt technique.

Strong assumption are needed to ensure convergence for sparse sequences of points. 

\subsubsection{Lipschitz continuity}
Function f does not have to change too fast, so it has a limited variability, or Lipschitz continuity. 

A function is L continuous if its change in value is bounded by the corresponding change in the evaluation point, multiplied by a constant factor $ K \geq 0 $.
$$
\mid f(x1)-f(x2) \mid < K \parallel x1 - x2 \parallel
$$ 

\subsection{Pure random search}
It is based only on repetitions of generated random points which have to be evaluated. They are generated usually with uniform distribution. 

Often used as component of more complex global optimization schemes. 

Convergence can be proved (soon or late, I reach the optimum using uniform dist), but rate of convergence is more interesting. Uniform probab distribution does not learn from previous history of search, so to speed up convergence we can combine a probab density Q with uniform prob distribution P, so that the convergence theorem still holds.

\subsubsection{Rate of convergence in PRS}
Different samples $ x_{j} $ are independent and identically distributed (i.i.d) with distribution p(x), and f has to hit the “target” set 
$$ 
B \big(x\ast , \epsilon \big) = \{ x \in A : \parallel x - x\ast \parallel \le \epsilon \}
$$

with point $ x_{j} $.

It can be a success or a failure, so it can be model with indipendent bernoully trials.

probab of failure is:
$$
Pr \{ x_j \notin B \} = 1-p(B)
$$

sample points are indipendent so I can multiply probab:

$$
(1-p(B))^n
$$

So average num of iterations required for a first hit of a ball B is:
$$
E(\text{first hitting time}) = 1 / p(B)
$$

Finally $ p(B) $ can be computed as the ratio between the vol of the ball B and the vol of the feasible reagion A:

$$ 
\frac{vol(A)}{vol(B)} = \frac{\pi^{d/2}*\epsilon^d}{\Gamma(d/2+1)*vol(A)}
$$

So now we can see that $ N\ast $ is:

$$
N\ast \approx -ln{\gamma} * \frac{\Gamma(d/2+1)}{\pi^{d/2}*\epsilon^{d}}*vol(A)
$$

Number of iteration strongly depends on the dimension \textit{d} of the search space. If \textit{d} increases, the number of iteration of PRS increase exponentially. 

The only thing we can hope is to deal with functions with regularities which can be learnt from an initial sampling, to identify shortcuts and get closer to the opt solution. 

Convergence does not mean anything in practice.

\subsection{Statistical inference in global random search}

The objective is to answer: what is the probability to get a new record value in the next n
iterations?  In some cases one needs a large number of sample points in the vicinity of the global optimizer in order to derive estimates.

Two application of stat inference are branch and bound and random multistart.

Branch and probab bound generalize branch and bound to continuous varibles.

Steps: 
\begin{itemize}
\item split set of admissible values in a tree-style :)
\item decide the potential value of individual subsets for other searches
\item select the most interesting subsets fo additional splits
\end{itemize}
	
Hyper-rectangles used for splitting space. Can be used for low-dim problems.

Random multistart repeats local search from random init points, until each search find a local min. We can use stat inf to estimate number of trials initial points to generate to guarantee that all loc min have been found. 

\subsection{Markov process and simulated annealing}

Local search can be modified in stochastic way. There are many ideas to do such a thing and they can be applied in continuous and discrete problems. 

Local search stops at a locally optimal point, searching in the vicinity of good local minima may lead to the
discovery of better solutions. I can accept worsening moves, hoping to reach in future better local min. The danger is to fall back again to the local min, because the next iteration cannot escape from the local basin. 

To avoid deterministic cycles I can use Simulated Annealing. I allow worsening moves but I still visit low \textit{f} values more often. I "shake" the function to visit worse point and then I try to find states with lower values than the initial one.

\subsection{Symulated Annealing and Asymptotics}
In PRS probab distribution for generating sample can depend on previously extracted points. A simplification is  to have the distribution just depend on the latest generated point and \textit{f} value. So, the sequence of sample points become a Markov chain, which has no memory on the history of previous search. This is not efficient.
SA is based on Markov but trajectory is built in ranomized manner: successor of current point is chosen stochastically, wiht probab depending only on the current point.

\begin{itemize}
\item{1. $x' \leftarrow \neighbor(N(X_j))$}
\item{2. $x_{(j+1)} = \begin{cases} 
      x' & \text{ if } f(x') \leq f(x_j) \\
      x' & \text{ if } f(x') > f(x_j) \text{ with probability } p = \exp{-\frac{f(x)-f(x_j)}{T}} \\
      x_j & \text{otherwise}
   \end{cases}
$}
\end{itemize}

SA introduces a temperature parameter T which determines the probability that worsening moves are accepted:
a larger T implies that more worsening moves tend to be accepted, and therefore a larger diversification occurs. The rule in the equation is called exponential acceptance rule.

If T is big (heating) , I obtain a random walk. If T is lowerd, I accept only improving moved (cooling).

\subsection{Asymptotic Convergence Results}

Many bad formulas to show that hen the number of iterations goes to infinity and the temperature is decreased slowly, the probability of
observing a global optimum goes to one but I have to wait toooooo loooong.

(in future there will be some tricks to use SA in online manner, learning the parameter T during search and adapting it in smart ways :) )

Actually, repeated local search, and even pure random search have better asymptotic results
for some problems.

\subsection{Inertial Shaker Algorithm}
The simpler Inertial Shaker (IS) technique, can be a practical choice to go beyond PRS while allowing on-the-job learning to rapidly adapt the probab of generating the next sample.

\begin{itemize}
\item probab is uniform over a search box defined around a current point x, where samples are generated. than, the search box is moved following the displacement applied to the point x
\item trend direction is computed averaging previous displacements
\item amplification and history depth are used to compute the displacement
\end{itemize}

Double shot strategy applied to all component of the vector of displacements. A displacement
is applied at every component as long as it improves the result. If no improvement is possible, then the function returns false, and the search box is accordingly shrunk.

\begin{figure}
\includegraphics[scale=0.60]{is}
\caption{inertial shaker alg}
\centering
\end{figure}


\section{Derivative Based Optimization}

Derivative based optimization techniques help solving problems on continuous variables. We will focus on two major related problems:
\begin{itemize}
\item{Solving a set of non-linear equations \\
	\begin{itemize}
	\item{ $ \text{Given } F : {\rm I\!R}^n \rightarrow {\rm I\!R}^n $ }
	\item{ $ \text{Find } x^\ast \in {\rm I\!R}^n \text{ such that } F(x^\ast) = 0 \longrightarrow \text{ if } x^\ast \text{ exists it minimizes } \sum_{i=1}^{n} f_i(x)^2 = 0 $ }
	\end{itemize}
}\bigskip
\item{
	Unconstrained minimization \\
	\begin{itemize}
	\item{ $ f : {\rm I\!R}^n \rightarrow {\rm I\!R}^n $ }
	\item{ $ \text{Find } x^\ast \in {\rm I\!R}^n \text{ such that } f(x^\ast) \leq f(x) \ \forall x \in {\rm I\!R}^n \longrightarrow x^\ast \text{ if global minima } $ }
	\end{itemize}
}
\end{itemize}
This chapter will collect some techniques to optimize smooth (derivable) functions on continuous variables. A function is derivable in a if exists the limit $$
f'(a) = \lim_{h\to0} \frac{f(a+h) - f(a)}{h} $$ If $x$ is very close to the point of interest $a$ ($h = x-a$), we can approximate the function in $x$ with $$ f(a+h) \approx f(a) + f'(a)*h $$ Like skis are tools for descending slopes, derivatives are useful tools to build local model and to minimize the objective function iteratively.

\subsection{Optimization and machine learning}

There is a deep relation between Optimization and ML because each one is used by the other to improve performances. ``Optimization for Learning'' is the art of choosing best model and variables configurations to let our machine learning system perform better. An example is to choose the best combination of weights in a NN. On the other side ``Learning for Optimization'' is the science of using learning techniques to improve the search of global minima that depends on the past steps done by the algorithm. This allows to build algorithms that have a memory of what they already did, for example to avoid visiting a non-interesting attraction basin more than once.

\subsection{Derivative based techniques in one dimension}
We start with a historical and fundamental method to find the point where the function is equal to 0, starting from a sufficiently close point and repeating the following steps:
\begin{itemize}
\item{Find a solvable local model}
\item{Solve the local model}
\end{itemize}
The local model can be found with:
\begin{itemize}
\item{the Taylor series approximation: $ f(x) = f(x_{c}) + f'(x_{c}) (x - x_{c}) + \frac{f''(x_{c}) (x-x_{c})^2}{2!} + ...$ }
\item{or the Newton's theorem: $ f(x) = f(x_{c}) + \int_{x_{c}}^{x} f'(z) dz \approx f(x_{c}) + f'(x_{c}) (x - x_{c}) $ }
\end{itemize}
The local model is therefore $$ M_{c}(x) = f(x_{c}) + f'(x_{c}) (x - x_{c}) $$ and so the next value $x_{+}$ will be $$ x_{+} = x_{c} - \frac{f(x_{c})}{f'(x_{c})} $$ 
If the function is linear, convergence occurs in one step, on the other case, it is possible to prove that if one starts close enough the the local minima, it will reach it in a fast (quadratic) manner.
\par
A global and safe algorithm to find roots of a function is the bisection method: if the function is continuous and we know at least two point $l_{0}$ and $r_{0}$ such that $f(l_{0}) < 0$ and $f(r_{0}) > 0$, the bisection method converges in a logarithmic number of steps. The cons is that it works only in one dimension.
\par
An other idea is to use backtracking: if Newton steps leads too far, one reverts the direction returning closer to the root. Finally, what usually expert do is to combine some powerful local convergence method (like Newton's) with others that are global and robust.
\par
Derivatives are not always available but can be approximated with secants. One can use the value of the previous iteration $f(x_{-})$ to find an approximation of the inclination: $$ a_{c} = \frac{f(x_{c}) - f(x_{-})}{x_{c} - x_{-}} $$ Given the less accuracy, convergence will be slower. 

\subsection{Solving models in more than one dimension}
Newton's method requires the gradient to be equal to zero. Given step $s$, the quadratic model is: $$Q(s) = \sum_{i=1}^{n} g_{i} s_{i} + \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij} s_{i} s_{j} \equiv g^{T} s + \frac{1}{2} s^{T} H s$$ The quadratic model is an reduction of the Taylor series to the second order, trying to approximate the objective function with a parabola in $n$ dimensions. Like many other times, we are searching a point where the gradient equals 0: $$ \nabla Q(s) = 0 = g +Hs $$ $$ Hs^{n} = -g \text{ (Newton equation)} $$ Because of the limited precision of real number on computers, one has to deal with numerical stability, so the computed value can be slightly different from the real solution. Ill-Conditioning is a term used to measure how much a function is sensitive to some errors in the data. There are cases in which very small oscillations in the input values lead to big changes in the output value of f, and has to be avoided.

\subsubsection{Gradient or steepest descent}
Often one has to work with many many dimensions and quadratic forms becomes inefficient. Moreover the Hessian H matrix is not always available and even an approximation in difficult to be computed. The base form of gradient descent updates the actual $x$ value in this way: $$ x_{+} = x_{c} - \epsilon \nabla f $$ This method has no global view of the actual situation, only local informations are available. A frequent problem arise with ill-conditioned matrices: when the quadratic form is ill-conditioned, the gradient inherits the computational errors and does not precisely point toward the local minima. So, it's important to design algorithms that do not ``zigzag'' toward the local minima but that try to keep a pretty linear trajectory, reducing so the time needed to reach the bottom of the valley.

\subsection{Non linear Optimization in more Dimensions}
The extension of the Newton's method in more dimensions consists in solving the following model: $$ m_{c} (x_{c} + p) = f(x_{c}) + \nabla f(x_{c})^T p + \frac{1}{2} p^T \nabla^2 f(x_{c}) p $$ Basically, given that we know the value and the gradient of the function in $x_{c}$, it is possible to approximate the value in close points $(x_{c} + p)$. If the initial point is close enough to the minimizer $x^*$ and the matrix $\nabla^2 f(x_{c})$ is positive definite, the method converges Q-quadratically to $x^*$. (If the Hessian H or it's approximation $\nabla^2 f(x_{c})$ are not positive definite, the method is not guaranteed to converge because the gradient is not guaranteed to point to the local minima).

\subsubsection{Global Convergence through Line Searches}
Global convergence is obtained by adopting line searches along the identified direction: one tries Newton's method first and then possibly backtracks.

\subsubsection{Secant methods}
Useful when Hessian H matrix is not available or too costly to be calculated. Given two near points, second derivative can be obtained calculating the ``variation of the gradient''. Like derivatives are given by the amount of change of the function in a given step $x_{2} - x_{1}$, second derivatives can be seen as the variation of first derivatives in $x_{2} - x_{1}$ (one-dimensional example):
$$ \frac{d^2 f(x)}{d x^2} (x_{2} - x_{1}) \approx \bigg( \frac{df(x_{2})}{dx} - \frac{df(x_{1})}{dx} \bigg) $$
In more dimensions one equation is not sufficient. Let the current and next point be $x_{c}$ and $x_{+}$, respectively, and let's define $s_{c} = x_{+} - x_{c}$ and $y_{c} = \nabla f(x_{+}) - \nabla f(x_{c})$ (difference of gradients). The analogous ``secant equation'' is
$$
H_{+} s_{c} = y_{c}
$$
Given that this equation has $O(n^2)$ admissible solution, past history is often used to choose the best one.

\subsubsection{Closing the GAP: second-order methods with linear complexity}
Computing the exact Hessian matrix (the one that contains the coefficients of the second order derivatives or ``the gradient of the gradient'') requires $O(n^2)$ time and $O(n^2)$ memory, in addition to the $O(n^3)$ complexity of finding next step in Newton method. OSS (One-step-secant) uses the info about the last gradient to approximate the new H matrix in linear time $O(n)$ with the secant method.
The new search direction $p_{+}$ is computed as follow:
$$
p_{+} = -g_{c} + A_{c} s_{c} + B_{c} y_{c}
$$
with $A_{c}$ and $B_{c}$ that weights first and second derivatives.

\subsection{Constrained Optimization and Lagrange Multipliers}
This section will shows briefly hot to deal with contraints, that is, with some restrictions on the input (features) space. A general constrained minimization problem is: 
$$
\min_{x}		f(x)$$
$$\text{subject to }		(g_{i} (x) = c_{i}) \ \forall i = 1,...,n \ \ \text{(Equality Contraints)}$$
$$			(h_{j} \geq d_{j}) \ \forall j = 1,...,m \ \ \text{(Inequality Contraints)}$$

Contraints are divided in two categories:
\begin{itemize}
\item{Hard Contraints, that must be respected at all cost (very rare in practice)}
\item{Soft Contraints, that can be violated paying a penality}
\end{itemize}
Soft Constraints are more interesting; an example of penalized objective function is:
$$
\min_{x} f(x) + \sum_{i} \gamma_{i} (g_{i}(x) - c_{i})^2
$$
In this case, the penalties increase quadratically with the ``error'' $(g_{i}(x) - c_{i} )$. If the function is smooth (derivable) and has partial derivates, this type of problem are usually solved with Lagrange multipliers:

$$
\min_{x} f(x) + \sum_{i} \lambda_{i} g_{i}(x)
$$ 

Minimizing the transformed function yields a necessary condition for optimality. Additional checks are therefore necessary (for example, the identified point can be a saddle point and not a global minimum), but in many cases, in the presence of a single global optimum, the method of Lagrange multipliers will deliver the correct solution. 

\section{Reactive Search Optimization (RSO): Online Learning methods}

The optimization function is considered as a black-box, but the more points are generated the more knowledge is accumulated in an implicit form. This data can be used to generate internal models and improve efficiency of our algorithms. RSO tends towards truly intelligent problem-solving machines, which learn and self-improve the more they work.
RSO combines machine learning techniques with optimization heuristics. Reactive means ready response to events during the search, through an internal feedback loop for online self-tuning and dynamic adaptation. Lot of machine learning methods like reinforcement learning and NN are used, trying for example to model the probability density function the target function.

\subsection{RSO: Learning while searching}
The main objective is to tune parameters automatically, like the temperature in SA. The intent is to transfer part of the human intelligence to the machine allowing the algorithm to work independently. It is very important that the following objectives are reached:
\begin{itemize}
\item{Reproducibility of results: given a complete and unambiguous documentation, everyone should achieve the same performance}
\item{Automation: tuning of the parameters should be automatic}
\end{itemize}
Given that real-worlds problem have a rich internal structure, this algorithm should learn on the job like the human brain, deriving future decision based on previous observations. Knowing why and when an algorithm is effective is a very valuable property. 

\subsection{RSO based on prohibitions}
The idea is to prohibit old solutions to be more creative and to reach unexplored new area. A we already mentioned, local search generates a trajectory $X^{(t)}$ of points in the admissible search space. The successor of a point $X^{(t)}$ is selected from a Neighborhood $N(X^{(t)})$. A point is locally optimal with respect to $N$ if $f(X^{(t)}) < f(Y) \ \forall \ Y \in N(X^{(t)})$. In this chapter we will consider a case in which $X$ is a binary string on length $L: X = \{0,1\}^L$ and the neighbourhood is obtained by applying elementary moves $\mu_{i}$ with $i = 1,...,L$ that change the \textit{i}-th bit of the string $X = [x_{1},...,x_{i},...,x_{L}]$. The algorithm work as follow: at each iteration $t$, we have:
\begin{itemize}
\item{The set of all possible moves $M^{(t)}$}
\item{The set of prohibited moves $T^{(t)} \in M^{(t)}$}
\item{The set of admissible moves $A^{(t)} \in M^{(t)}$}
\end{itemize}
The next point is given by:
$$
X^{(t+1)} = \mu^{(t)}(X^{(t)}) \text{ where } \mu^{(t)} = \argmin_{v \in A^{(t)}} f(v(X^{(t)}))
$$
Basically, the admissible move that leads to the best point is chosen. But how do we populate the set of prohibited moves $T^{(t)}$? It depends on the case. For example, to avoid cycles, one can prohibit a move for a period $\tau$ after having executed it. If $\tau$ time has passed from the last time move $\mu_{i}$ was executed, then it will be newly available. Prohibit moves for a finite amount of time will be fundamental in later phases to reach the global minima. In the string case, this means that after having flipped a bit we should wait at least $\tau$ steps to flip it again. $\tau$ should be learnied in a way that there will be the possibility to reach a new attraction basin in $\tau + 1$ steps. Prohibitions methods are effective on combinatorial optimization and sub-symbolic machine learning.

\subsection{Fast data structures for using the search history}
Access past events of the trajectory $X^{(0)}, X^{(1)}, ..., X^{(t)}$ is mainly done through hashing functions and radix-tree techniques. A first technique combines hashing with linked lists: each bucket of the hash-table points toward a linked list of all elements in that bucket. Each item can be retrieved in approximated constant time if the number of buckets if sufficiently large and the hash functions distributes value with a pretty uniform probability.

An other method consists in combining hashing with persistent red-black trees. This data structure will not be ephemeral (previous version will not be destroyed when inserting / deleting data) but will be partially persistent. RB trees are used to do insert/delete of elements in $O(\log n)$ and can return all their content in $O(n)$. Each RB tree stores a copy of the system at step $t$ and all trees are reachable through a hash-table. If more configurations $X^{(t)}$ have the same hash, they are stored in a linked list. Each element of the list will point to a different RB. This method allows us to:
\begin{itemize}
\item{Store all the trajectory of our algorithm}
\item{Retrieve each config in $O(1) \text{ (time for the hash) } + O(n) \text{ (time to a DFS of the tree) }$ }
\item{Add a new configuration incrementally, trying to use previously computed subtrees}
\item{Share subtrees between different RB trees to be efficient}
\end{itemize}

\section{Adapting Neighbourhoods and Selection}
Possible moves are collected into a set of different
neighborhoods (NH). During local search I have to select a 
NH and how to select one neighbor to be the next poin in the search trajectory.

The first is the most problematic issue, but we can use online learning strategies to solve both of them.

In first case I can decide NH a priori and selecting the neighbor is dynamic. The longer I evaluate the neighborhood, the better the chance of
identifying a move with a large improvement, but the shorter the total number of moves which one can execute in a slot of CPU time.

\subsection{VNS}
Fixing the NH a priori is not a good choice, I have to adapt it to the local config around the current point. 

I consider a set of NG, defined a priori and I want to use the most appropriate one during search. 

So VNS has to:

\begin{itemize}
\item understand which NH use and how many
\item how to schedule them (order?)
\item how to select neighbor inside the NH (best one? the first one found?)
\end{itemize}

The first issue can be decided based on detailed problem knowledge and preliminary experimentation.

The second issue can be solved in different manner. I can cycle randomly among the NH (no online training), or I can change NH when I reach a local min inside the current NH. 
As soon as a local minimum is reached for a specific NH, improving moves can be found in other NH. Online training is used because I have to change NH using the minimal diversification I need.

The third issue can be addressed in two ways: best improvement local search and first-move. The first method considers all neighbors (brute-force) and evaluate them, then I move to the one with the best value. The second one considers only a sample of possible moves, a subset of neighbors, so, finally I return the first candidate with better \textit{f} value. This method is adaptive because the exact number of neighbors evaluated before deciding the next move depends on the local properties in the config space around the current point.
I evaluate a small number of candidates in the early phase of the search, and identifying
an improving move will become more difficult during the later phases, close to local optimality.

As soon as search is successful, back to default NH.

VN Descent uses the defaul NH first and the other only if it fails (current point is local min for it). I can order NH according to the strength of perturbation.
Reduced VNS generate a random neighbor before moving or not, so in the selection of neighbor alg I find this line:

\begin{figure}[H]
\includegraphics[scale=0.60]{random}
\centering
\end{figure}

Skewed VNS moves also in worsening zones if the lead to search trajectory sufficiently far from current point (I need also distance function to control diversification and a parameter to regulate tradeoff movement distance/acceptance worse moves).

\begin{figure}[H]
\includegraphics[scale=0.50]{vns}
\caption{VND and skewed version}
\centering
\end{figure}

\section{Iterated Local Search}
We assume that an LS algorithm is available as a black-box. The first and simpler method consists in starting LS from a set of point uniformly distributed over the input space $X$. As usually, the problem with this method is that it does not learn while searching a local or global minima. The right way is to use history to produce better starting point, taking in account the assumption that local minima are usually clustered. Moreover, many problems allow to evaluate $f$ incrementally from close input values.
Incremental local search works as follow:
\begin{itemize}
\item{Perturb current local minima}
\item{Start local search from the modified solution}
\end{itemize}
As you may have noticed, it is similar to VNS. This simple but effective method is better than restarting always from a casual new point because from the law of large numbers, $f(X^{(t)})$ values tends to concentrate local minima around an average $\E[f(x^*)]$ after lot of trials. Starting from a new point close to the last local minima destroys the independency of the events, trying to skip the law of large numbers.

A more robust technique works as follow:
\begin{itemize}
\item{Find some local minima (e.g. launching local search from some random point)}
\item{Restart the search from this local minima in their neighbourhood}
\end{itemize}
This approach creates a hierarchy of nested local searches. Usually, the level of nesting is two (like in the example). An important question is: how do we find neighbour attraction basins? Create a perfect map of attraction basins is very difficult for obvious reasons. A simple method to reach a neighbour attraction basin consists in using perturbations: start with a weak perturbation and increase the power until a new attraction basin is found. To avoid cycles, use some randomness in the generation of perturbations. Strength history should be used to speed up the process of finding how strong the new perturbation should be.
Picture \ref{fig:ils} shows how IteratedLocalSearch works.

\begin{figure}[H]
\includegraphics[scale=0.25]{ils}
\caption{Iterated Local Search}
\centering
\label{fig:ils}
\end{figure}

\section{Online learning in SA}
A good thing in SA is the asymptotic convergence: if num of iterations is infinite and the temperature (T) parameter decreases slowly (cooling phase) the probab of find global opt is 1. Drawback is that if T is too small wrt the step of \textit{f} necessary to escape from the attraction basin, I keep on cycle around the attractor. This happens because SA has no memory of past local search. 

\begin{figure}[H]
\includegraphics[scale=0.40]{sa}
\caption{SA: T is too low wrt jump size so many iterations spent in the valley :( }
\centering
\end{figure}

If I add online learning I can improve SA. I can estimate progress, entrapment region and then activate ways to escape from these situations, like increasing the T parameter.

\subsection{combinatorial opt problems}
In traditional SA I have 3 parameters: $ T_{start}, T_{end}, T_{t+1} = \alpha * T_t $. So, I have to choose the T parameters and the $ \alpha $, but the scale of T may be wrong, leading to poor results. It is suggested to estimate the \textit{f} values distribution in order to understand at which value fix the $ T_{start}, \text{ and the } T_{end} $ parameters: $ T_{start} = \sigma^2 f $ and $ T_{end} = \text{ min } f \text{ variations} $

Great T values correspond to wide variations of \textit{f}, also called reconfigurations. In physic world it is called phase transition. I reach this situation when the specific heat is max. After big reconfigurations there is a cooling phase (T is decreased), using two parameters: $ \alpha $ for fast decreasing and $ \beta $ for slower decreasing. one switches from a faster temperature decrease given by $ \alpha $ to the slower one given by $ \beta $.
After the initial period the temperature is very low: the system freezes, so no tentative moves are accepted. In many case I need an anytime algorithm sot that longer allocated CPU times allow to reach better values until user decides to stop. They return the best answer possible even if they do not run to completion, and they may improve on the result if they are allowed to run longer. Stopping criterion should be decided a posteriori, after estimating additional time cannot reach better results. 

This problem is related to monotonic decrease, so I should fix the reset T to a constant T, high enough to escape the local min but also low to visit them. I can fix it to the T that I found when the best heuristic sol is found in a preliminary SA run. 

This is non-monotonic cooling:

\begin{itemize}
\item exploit an attraction basin rapidly
by decreasing the T so that the system get close to the local minimizer
\item increasing the
T to diversify the sol and visit other attraction basins
\item decreasing again after reaching a different
basin
\end{itemize}

The T increases in this kind of non-monotonic cooling schedule has to be rapid enough to avoid falling back to the current local min, but not too rapid to avoid a random-walk situation (where all random moves are accepted).

Possibilities to increase the temperature include resetting the temperature to $ T_{reset} = T_{found} $, the T value when the current best solution was found. 
In the cooling phase, if the reset is succesful I can reduce the reset T by halving it.
Alternatively I can multiply T by a heating factor $ \gamma > 1 $. $ \gamma $ should be choose with learning process: it should be near to 1 in the first phase, if the system is trapped, it has to be increased to escape from the basin.

Experimental evidence shows that the a priori determination of SA parameters and acceptance function does not
lead to efficient implementations. A simple adaptive technique is the SEQUENCE HEURISTIC: a perturbation leading to a worsening solution is accepted if and only if a fixed number of trials could not find an improving perturbation.

\subsection{SA for global opt of continuous functions}
I can apply SA to continuous opt. 

\begin{itemize}
\item generate a new point with a random step along a direction $ \textbf{e}_h $
\item evaluate the function
\item accept the worsening move with a certain probability
\end{itemize}

First choice is the length of the random step along $ \textbf{e}_h $, I  have to learn from the local \textit{f} surface, so the new trial point $ \textbf{x'} = \textbf{x} + Rand(-1,1)v_h \textbf{e}_h $, where $ Rand(-1, 1) $ returns a random number uniformly distributed between -1 and 1,  $ \textbf{e}_h $ is the unit-length vector along direction h, and $ v_h $ is the step-range parameter, one for each dimension h, collected into the vector \textbf{v}.

In Adaptive Simulated Annealing (ASA) the parameters that control the temperature cooling schedule and the random step selection are automatically adjusted according to algorithm progress.

\section{Dynamic search and noise level}
A strategy to search for global opt can be modify in a reactive way the function \textit{f}, in order to make local good areas to appear less feasible. For example, I can increase the value of the function in an area of local min, in order to encourage the local search procedure to discard keeping on searchin in that zone. 

GSAT is an algorithm for satisfiability of boolean formulas that uses multiple runs of local serach. At each iteration, the bit which gives the max current number of clauses is flipped, in order to find other different configurations of variable which allow to reach better results. 

\begin{itemize}
\item I have a number of tentatives
\item for each tentative I can run a number of iterations
\item for each iteration a variable is chosen, using 2 different criteria: the variable in an unsatisfied clause or the variable which allow to reach the max number of satisfied clauses. 
\item the chosen var is flipped
\item I need to save the truth assignment with give the max number of sat clauses
\end{itemize}

To avoid stopping at local optimum I can give clauses a different weight, for example I can give more weight to clauses to hide local optima. This techinque is reactive because current weights depends on the local characteristics. 

An alternative is to learn from the most difficult sat clauses. The most difficult clauses to be satisfied are those which remain unsat after many trials. So, I can increas their weight to increase their priority in the local search phase. Whend picking a move, they will be considered more than the other clauses. 

Now I have a problem, because I can continue increase some clause weights, obtaining that their size is too big wrt to others. A strategy can be lowering their weight once they become sat, or having a decayment factor which has to reduce their weight. 

With these techiniques I applied global changes, this can be a problem because it can affect also further points with similar values in the search space.

I can use two parameters, one for sat clauses and one for unsat clauses and then smooth them using the mean. (Scaling And Proab Smoothing).

\subsection{guided local search}
A similar approach is proposed for other applications and the aim is to guide a local search algorithm in the search space. 
This is the scheme proposed

\begin{itemize}
\item features penalties
\item costs of features (known a priori, as cost edges in TSP)
\item NH activation scheme depending on the current state 
\end{itemize}

So I can use an augmented cost function $ h(X) $ defined as:

\begin{figure}[H]
\includegraphics[scale=0.50]{augmented}
\centering
\end{figure}

where $ I_i(X) $ is an indicator function which returns 1 if feature  \textit{i} is present in X, 0 otherwise.The augmented cost
function is used by local search instead of the original function.
	
Penalties are initially set to 0, they are updated lately. $ \lambda $ and penalties are adapted in online manner. 

When a local minimum of h is encountered the augmented cost function is modified by updating the penalties $ p_i $ . One considers all features $ f_i $ present in the local min solution X' and increments by one the penalties which maximize:

\begin{figure}[H]
\includegraphics[scale=0.50]{penalties}
\centering
\end{figure}

First a higher cost $ c_i $ , and therefore an inferior a priori desirability for feature $ f_i $ in the solution, implies a higher tendency to be penalized.
Second, the penalty $ p_i $, which is also a counter of how many times a feature has been penalized, appears at the denominator, and therefore discourages penalizing features which have been penalized many times in the past.

I can combine GLS with Fast LS, partitioning the neighborhoods in sub areas, which can be activate or disactivated according to the search process. If a NH does not contain improving moves, it is disactivated, if some improving moves can be found inside a NH, this is activated and used in the search process. 

A difficult thing in GLS which has to be managed is the diversification of unit measures. Moreover, penalties can modify the fuction too much, avoiding to find some local interesting min, skipping some solutions.

\subsection{adapting noise levels}
I can add some random noise to moves, in order to influence diversification. So, I can use a noise parameter to determine a random variable which has to be flipped in GSAT. This parameter can be set a priori, after some preliminary trials or adapted online. 

I always have to search a compromise between intensification of search and diversification, and this noise parameter allow me to add a controlled randomization on moves.

\section{Adaptive Random Search}
This methods apply when partial or complete derivates are not available. Adaptive Random Search is based only on the knowledge of the visited points. The base version works as follow:
\begin{itemize}
\item{1. Choose initial point and initial search region;}
\item{2. Generate new candidate point $x_+$ with a certain probability distribution on the search region;}
\item{3. Search region is expanded if $f(x_+) < f(x_c)$, otherwise is compressed ($x_c$ previous point);}
\item{4. If $f(x_+) < f(x_c)$ then $x_+$ is new current point and search region is moved around it;}
\end{itemize}
For effective implementations, simple search region around the current point suffice. An example is using boxes: 
$$
\delta = \sum_{j} Rand(-1,1) * b_j
$$
 with $b_j$ as base vector. Note that this search region is not a sphere but a hyper-rectangle.

\subsection{RAS: adaptation of the sampling region}
RAS stands for Reactive affine shaker is a method to modify the search region in an online manner. RAS use an affine transformation to 
create the new search region. An affine transformation is the composition of a linear one followed by a translation (e.g. rotation, scaling, shifting, shear, ...) :
$$
x \rightarrow Ax + b
$$
An affine transformation has the following good properties:
\begin{itemize}
\item{Points which lie on a line continue to be collinear after the transformation}
\item{Ratios of distances along a line are preserved}
\end{itemize}

This algorithm tries to minimize the number of steps needed to reach the local minima. By compressing the search region only if next point is worse, this algorithm tries to do always the largest possible movement. A pseudocode example is shown in picture \ref{fig:ras}. $\Delta$ is a vector generated with a uniform probability distribution over the search region. The algorithm uses a double-shot strategy: if adding $\Delta$ does not lead to a better point, it will try to go in the opposite direction. Using a double-shot strategy heavy reduce the probability of generating sequences of unsuccessful events. At each iteration a matrix P is computer and each base vector $b_j$ is updated.
If the function found a new better point, P will increase the length of base vectors, otherwise it will compress the search region. $\rho > 1$ is the expansion ratio. 

\begin{figure}[H]
\includegraphics[scale=0.25]{ras}
\caption{Reactive Affine Shaker}
\centering
\label{fig:ras}
\end{figure}

\subsection{Repetitions for robustness and diversification}

In general, estimate the number of steps required for identifying a global minimum is impossible. RAS can be stopped for example when search region become smaller that a certain threshold. To be more robust, one can launch different instances of RAS starting from different random points, creating a sort of population of independent RAS searchers. More complex methods to coordinate them are showed in Chap 15.

\section{Linear and quadratic programming}

In LP I have a linear objective funtions subject to linear constraints, and I have to found input values which can maximize this function respecting also the constraints. 

Integer LP is a version of LP with the additional requirement that input values must be integers. 

Quadratic Programming can be used to solve efficiently problems with quadratic objective functions and linear constraints.

\subsection{Linear programming (LP)}
I have to optimize the linear obj functions so that linear inequality/equality constraints are satisfied. 

If x is one-dimensional, a linear function is a straight line, constraints are inequalities like $ a \leq x $ or $ x \leq b $. If there are values of x satisfying the inequalities, and the slope of the line is different from 0, an optimal solution will be one of the boundary values (a or b). 

Attention: constraints like a < x are not acceptable (imagine f(a) is the minimum value). In fact, given a point close to a, we could always find a smaller x value with a smaller f value: one needs compact sets, closed (containing all its limit points) and bounded.

Linear programming is a technique for the optimization of a linear
objective function, subject to linear equality and linear inequality constraints. LP can be expressed in canonical form
as

\begin{figure}[H]
\includegraphics[scale=0.50]{lp}
\centering
\end{figure}

The inequalities $ Ax \leq b $ and $ x \leq 0 $ are the constraints which specify a geometrical figure known as convex polytope over which the objective function has to be optimized.
Each constraint, (the scalar product between a row of matrix and the vector of variables: $ A_i * x \leq b_i $) , defines a half-space of points satisfying it. Because all constraints have to be satisfied, the intersection of the half-spaces (when bounded and non-empty) defines the convex polytope. Convexity is important.

Polytope contains an infinite number of points, so brute-force algorithms are impossible.

An optimal solution can always be found at a vertex. For a demonstration, convexity and linearity are the keys.

The feasible region is convex and the objective function is linear, so, a local optimum is a global optimum. 

Demonstration:
 
If the above does not hold, there will be a point $ x_loc $ which is locally optimal but with a global optimum point $ \textbf{x}_opt = \textbf{x}_loc $ of higher objective value. If one considers the line segment connecting the two points, because of convexity belonging to the feasible region, the objective along this segment will increase. In particular, it will increase also for points along the segment in all local balls surrounding the local opt, a contradiction with the fact that is is locally opt.

In addition, a global optimum can always be found among the vertices.

Proof: 
Assume that a local optimum $ x_opt $ is in the interior of the LP polytope. If the gradient of the linear objective function is non-zero, one can move away from the local opt along the line $ \textbf{x}_opt + t*\textbf{c} (t \leq 0) $ and obtain higher objective values, until a point on the boundary is met. If the point on the boundary is not a vertex, one considers the vectors spanning the facet or edge. Either the gradient is perpendicular (in which case moving along the faces will not change the objective), or its projection defines a direction to follow to get even higher objective values. 

In all cases one can safely move (getting higher or equal objective values) until a vertex is reached.

By the above conclusions that local opt is sufficient one derives the \textbf{simplex algorithm} for LP:

\begin{itemize}
\item It starts at some vertex. If the problem is solvable (the feasible region is not empty), an initial vertex can be determined by applying the simplex algorithm to a modified and easily solvable version of the original program. 
\item A sequence of local search (LS) steps are executed, in which the possible neighbors of the current vertex are the vertices that can be reached by moving along an edge. LS always moves to an improving local vertex (one with higher obj value).
\item Simplex terminates when it reaches a local max, a vertex from which all neighboring vertices have a smaller objective value. Because of convexity of feasible region and linearity of objective function, this is actually a global opt.
\end{itemize}

Convergence? this simple version has to be cured to avoid possible cycles (case of neighboring vertices with equal objective values). 

Complexity? The existence of solutions algorithms for LP with guaranteed polynomial complexity has been an open problem in computer science for many years. The LP problem was first shown to be solvable in polynomial time by in 1979.

\subsubsection{algebraic view of linear programming}
A first observation is that the boundaries of the polytope corresponds to some inequalities in the constraints becoming equality. 

Dealing with equalities only is simpler than dealing with inequalities. Luckily, one can transform an LP problem into the slack form:

\begin{figure}[H]
\includegraphics[scale=0.60]{alp}
\centering
\end{figure}

by a simple trick of adding additional slack variables. For each constraint:

\begin{figure}[H]
\includegraphics[scale=0.60]{slack}
\centering
\end{figure}

one introduces a new variable s and rewrites the above inequality as the two contraints:

\begin{figure}[H]
\includegraphics[scale=0.60]{ineq}
\centering
\end{figure}

\subsection{Integer Linear Programming} 
An Integer Linear Programming problem (ILP) is an LP problem with the additional constraints that the variables \textbf{x} must take on integral values. ILP is NP-hard. The feasible region is not a nice  polytope anymore but a set of dots corresponding to integer (feasible)
coordinates. So it's impossible to move from vertex to vertex continuously.

A heuristic way to proceed is to simply remove the constraint that \textbf{x} is integral, and solve the corresponding LP (called the LP relaxation of the ILP). The optimal LP value is an upper bound (when one relaxes to real values, and therefore has more possibilities to improve a solution). One can then round the entries of the solution to the LP relaxation to the nearest integers. Of course, a solution obtained
by rounding may not be optimal, it may not even be feasible (it may violate some constraint). ILP problems can be solved with heuristic local search techniques.

\subsection{quadratic programming}
The problem of optimizing a quadratic function of several variables subject to linear constraints on these variables. An example is Support Vector Machines.
For positive-definite Q, the ellipsoid method solves the problem in polynomial time. But if Q is indefinite, the problem
becomes NP-hard.

\begin{figure}[H]
\includegraphics[scale=0.60]{qp}
\centering
\end{figure}

\section{Branch and Bound and Dynamic Programming}
The most relevant algorithmic patterns in addition to greedy and local search are Branch and Bound and Dynamic Programming.

Branch and bound is a little better than the exhaustive enumeration of all possible solutions.

Dynamic Programming is based on decomposing a problem, finding solutions to smaller instances, and re-using them to build solutions to larger and large instances, in a bottom-up manner. Because smaller instances are found many times, it makes sense to store their solutions in an organized memory structure.

\subsection{Branch and Bound}
If one needs to find a best configuration out of a finite set, a brute-force algorithm is exhaustive search: generate all possible configuration and evaluate the corresponding objective-function values.

If a configuration corresponds to picking specific values for a set of variables, the entire set of configurations can be organized as a solution tree: the root is the starting situation.
Complete solutions correspond to the leaves of the tree, while internal nodes are partial solutions. Exhaustive search then is implemented by generating the entire tree and examining its leaves.
 
The number of leaves can grow exponentially. To save CPU time I can delete parts of the trees that cannot lead to any improvement (pruning). I can demonstrate that some partial solutions have no hope of becoming optimal when completed.

An algorithm design paradigm is to associate nodes with sets of solutions, the complete slutions which can be obtained by fixing the remaining free variables in all possible ways. The full set of candidate solutions is at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. 

Branching means assigning a value to a variable in a partial solution and finding, in a recursive manner, the best value in the corresponding subtree. 

Before enumerating the candidate solutions of a branch, the branch is checked against an optimistic bound on the optimal solution, and
is discarded if it cannot produce a better solution than the current record value (the best value found so far by the algorithm).

The advantage w.r.t. exhaustive search depends on the existence, quality and speed of computation of the optimistic bound used for pruning. If the bound is tight, a big fraction of the search space can be cut.

Branch-and-bound aims at minimizing the value of a real-valued function $ f(x) $, in which $ x \in S $, the set of admissible, or candidate solutions, (search space).

The alg works as follow:

\begin{itemize}
\item It recursively splits the search space into smaller spaces (branching), then minimizing $ f(x) $ on these smaller spaces.
\item It keeps track of an optimistic bound on the minimum value which can be reached by completing a given partial solution in all possible ways. If the bound is larger than the current record value, the current subtree is pruned, and control is returned to the first encountered higher level in the tree which has alternative ways to fix variables
not yet explored (backtracking).
\end{itemize}

Branch And Bound is unusable for most real-world problem due to its CPU time requirements: nodes explode exponentially as the problem size increases.

\subsection{Dynamic Programming}
The principle is solving problems by combining solutions to subproblems, with the specific characteristic that the subproblems are not independent, but problems share subproblems. DP solves each subproblem only once and then saves its result in a table, avoiding useless re-computation.

\begin{itemize}
\item Study and characterize the structure of an optimal solution (how it contains solutions to smaller instances)
\item Define the value of an optimal solution in a recursive manner from the values of solutions of subproblems.
\item Compute the value of the optimal solution in a bottom-up manner (by starting from the smallest subproblems)
\item Construct an optimal solution from information computed while finding the value
\end{itemize}

Dynamic Programming is the base of Viterbi's algorithm. To help the intuition, let’s consider a simplified problem of text recognition.
The probability of recognizing a character at a certain position depends on the underlying sequence, and on the context given by additional characters around it. 
We have simple models able to calculate probabilities of complete sequences. The principle is searching for the most probable sequence given the observed signals and given a model of sequence generation.

Very useful and used models of sequences generated by an underlying hidden process are \textbf{hidden Markov model (HMM)}.

The intention is to model a system with a hidden state (not directly measurable) which produces one among a set of output signals (or symbols), with a probability for each symbol. 
>>>>>>> e324923d968febe58837913805af536d9cf9bb7a

Mathematically the hidden Markov model is characterized by 
\begin{itemize}
\item a state space S
\item initial probabilities $ \pi_i $  of being in state \textit{i}
\item transition probabilities $ a_{i,j} $ from state \textit{i} to state \textit{j}
\item $ b_{i,o} $ emission probab that stat \textit{i} emits observation \textit{o}
\end{itemize}

we observe outputs $ y_1, ..., y_T $.

$ V_{t,k} $ is the probab of the most probab state sequence ending at time \textit{t} at state k.

So, assume that $ (s1, ... s{t}, k) $ is the optimal sequence ending in state k at time \textit{t}, and we need to consider the sequence at the iteration before.

Now, consider the most proable sequences stopping at iteration before, \textit{t-1}. $ V_{t-1, x} $ is the probab of the most probable path at the previous step in state x. 

Two events must happen to reach k, a transition from x to k and the emission of the measured signal $ y_t $.

Because we are searching for the most probable sequence, we must maximize the probability over all possible previous points x.

So the most likely sequence $ x_1, ... x_T $ is given by

\begin{figure}[H]
\includegraphics[scale=0.60]{V}
\centering
\end{figure}

the Viterbi path can be retrieved by saving back pointers that remember which state x was the winning one in the second equation.

$ Ptr(k,t) $ returns the value of x used to compute $ V_{t,k} $ if t > 1, or k if t = 1. Then:

\begin{figure}[H]
\includegraphics[scale=0.60]{x}
\centering
\end{figure}

The complexity of this alg is $ O(T \text{ X } |S|^2) $, while a complexity of a brute-force scheme considering all possible paths is $ O(|S|^T) $.


\section{Satisfiability}
The satisfiability problem is one of the most important in computer science. It's a well known NP-Complete problem and consists in finding an assignment to all the variables of the boolean formula such that the whole formula is True. This problems are really simple to be understood and really difficult to be solved.

\subsection{Satisfiability and maximum satisfiability: definitions}
One is given a boolean formula in CNF, a conjunction of clauses. The problem consists in finding an assignment that satisfies the max number of clauses. If a formula has $n$ variables and $m$ clauses, it can be written as:
$$
\bigwedge\limits_{1 \leq i \leq m} ( \bigvee\limits_{1 \leq k \leq |C_{i}|} l_{ik})
$$
The set of clauses is called $\boldsymbol{C}$. If one assigns a weight $w_i$ to each clause, the problem because MAX W-SAT. If each clause contains a fixed number k of literals, the problem becomes MAX-k-SAT.

\subsubsection{Notation and graphical representation}
MAX-SAT is easy to define and excellent to visualize. Given a formula $ (u_1 \lor \lnot u_3 \lor u_5) \land (\lnot u_2 \lor \lnot u_4 \lor \lnot u_5 ) \land ( \lnot u_1 \lor u_3 \lor \lnot u_4 )$ the respective table is showed in Picture \ref{fig:bool}.
Truth values to variables are assigned by placing a black triangle to the left if the variable is true, to the right if it is false. Each literal is depicted with a small circle, placed to the left if the corresponding variable is true, to the right in the other case. If a literal is matched by the current assignment (e.g., if the literal asks for a true value and the variable is set to true, or if is asks for false and the variable is false), it is shown with a gray shade. The coverage of a clause is the number of literals in the clause that are matched by the current assignment, and it is illustrated by placing a black square in the appropriate position of an array with indices ranging from 0 to the number of literals in each clause $|C|$.

\begin{figure}[H]
\includegraphics[scale=0.5]{bool}
\caption{Table of a boolean formula}
\centering
\label{fig:bool}
\end{figure}


\subsection{Resolution and Linear Programming}
\subsubsection{Resolution and backtracking for SAT}
Inspired by Branch and Bound adapting it to the structure of SAT. A basic tool we will need is that of resolution, given by the recursive replacement of a formula by one or more formulae, the solution of which implies the solution of the original formula. In resolution a variable is selected and a new clause, called the resolvent is added to the original formula. The process is repeated to exhaustion or until an empty clause is generated. The original formula is not satisfiable if and only if an empty clause is generated. 

A clause R is the resolvent of clauses $C_1$ and $C_2$ iff there is a literal $l \in C_1$ with $\lnot l \in C_2$ such that $R = (C_1 \setminus \{l\}) \cup (C_2 \setminus \{l\})$ and $u(l)$, the variable associated to the literal, is the only variable appearing both positively and negatively. For the two clauses $C1 = (l \lor a_1 \lor ... \lor a_A)$ and $C2 = (\lnot l \lor b_1 \lor ... \lor b_B)$ the resolvent is therefore the clause $R = (a_1 \lor ... \lor a_A \lor b_1 \lor ... \lor b_B )$. Is we add the resolvent as a new clause, the solution does not change, because if both $C_1$ and $C_2$ are satisfied, they have at least one literal in common that will solve the resolvent. If there is not common literal between $C_1$ and $C_2$, then they contain only $l$ and $\lnot l$ and so are not both satisfiable. Picture \ref{fig:resolvent} shows how to construct a resolvent.

\begin{figure}[H]
\includegraphics[scale=0.5]{resolvent}
\caption{How to construct a resolvent}
\centering
\label{fig:resolvent}
\end{figure}

One of the best SAT solvers is the DPLL algorithm. It's pseudocode is showed in Picture \ref{fig:dpll}.

\begin{figure}[H]
\includegraphics[scale=0.5]{dpll}
\caption{DPLL algorithm}
\centering
\label{fig:dpll}
\end{figure}

with as input a set of clauses $\boldsymbol{C}$. Remember that:
\begin{itemize}
\item{Pure literal: If a propositional variable occurs with only one polarity in the formula, it is called pure. Pure literals can always be assigned in a way that makes all clauses containing them true. Thus, these clauses do not constrain the search anymore and can be deleted.}
\item{Unit clause: If a clause is a unit clause, i.e. it contains only a single unassigned literal, this clause can only be satisfied by assigning the necessary value to make this literal true. Thus, no choice is necessary. In practice, this often leads to deterministic cascades of units, thus avoiding a large part of the naive search space.}
\item{$\boldsymbol{C}(l)$ means replacing the LITERAL $l$ with True in all clauses. If the literal is a negated variable, then it will be replaced with False.}
\end{itemize}

Unfortunately, in the worst case, this algorithm will require an exponential number of steps to find out if the formula is satisfiable. 

\subsubsection{Integer programming approaches}
The MAX W-SAT problem has a natural integer linear programming formulation. Let $y_j = 1$ if boolean var $u_j$ is true otherwise $0$. Let $z_i = 1$ if clause $C_i$ is satisfied else $0$. The integer linear programming problem is:
$$
\max \sum_{i=1}^{m} w_i z_i
$$
subject to the following constraints (it says that each clause should have at least a true assignment to a literal):
$$
\sum_{j \in U_i^{+}} y_j + \sum_{j \in U_i^{-}} (1 - y_j) \geq z_i , \ \ i = 1,...,m
$$
with $U_i^{+}$ and $U_i^{-}$ as the sets of positive and negative variables in clause $C_i$. With this formulation, a simple Branch and Bound method should be able to solve the problem and maximize it. Like in DPLL, from the root two branches are generated, one with a true assignment to a chosen variable and the other with a false assignment (last two lines of the algorithm).

\subsection{Continuous approaches}
Other methods to solve ILP is to solve a related problem of inductive inference, in which one aims at identifying a hidden boolean function using outputs obtained by applying a limited number of random inputs to the hidden function. The task is formulated as a SAT problem, which is in turn formulated as an ILP:
$$
A^T y \leq c, \ \ y \in \{-1,1\}^n
$$
where $A^T$ is an $m \times n$ real matrix and $c$ a real vector of length $m$. The so called interior point is based on finding the local minimum in the box $-1 \leq y_j \leq 1$ of a potential function $\phi(y)$. If there is a solution, it will be the solution of the ILP problem too.

\subsection{Approximation algorithms}
MAX-SAT is a playground for algorithms with guaranteed quality of approximation. The basic principle is to guarantee that the delivered result will be within a certain percentage of the optimal solution. The first approximate algorithm for MAX W-SAT were proposed by Johnson and is showed in Picture \ref{fig:johnson}.

\begin{figure}[H]
\includegraphics[scale=0.5]{johnson}
\caption{Johnson algorithm}
\centering
\label{fig:johnson}
\end{figure}

The Johnson algorithm can be showed to be $\frac{k}{k+1}$-approximate for MAX-SAT. An improved version, that is Johnson2, remembering that $k$ is the minimum number of literals in a clause. Picture \ref{fig:johnson2} shows the second version, that is $(1 - \frac{1}{2^k})$-approximate algorithm. 

\subsubsection{Randomized algorithms for MAX W-SAT}
A randomized $\frac{1}{2}$-approximate algorithm for MAX W-SAT. One of the most interesting approaches in the design of new algorithms is the use of randomization. During the computation, random bits are generated and used to influence the algorithm process. First see the algorithm \textit{RANDOM} in Picture \ref{fig:randomsat}.

\begin{figure}[H]
\includegraphics[scale=0.3]{randomsat}
\caption{The Random algorithm}
\centering
\label{fig:randomsat}
\end{figure}

The \textit{RANDOM} algorithm is $(1 - \frac{1}{2^k})$-approximate, and with a $k$ that can potentially be 1, we say it is $\frac{1}{2}$-approximate. It is easy to proof that if a clause has k literals, satisfiying at least one of them has probability  $(1 - \frac{1}{2^k})$.  An improved version, that differers only in the probability of setting variables to true, is shown in Picture \ref{fig:genrandom}.

\begin{figure}[H]
\includegraphics[scale=0.3]{genrandom}
\caption{The GenRandom algorithm}
\centering
\label{fig:genrandom}
\end{figure}

\textit{GenRandom} can be shown to be $\frac{3}{4}$-approximate for MAX W-SAT.

\subsection{Local Search for SAT}
MAX-SAT is among the problems for which perturbative local search is very effective: the general base scheme is based on generating a starting point in the set of admissible solution and trying to improve it through the application of simple basic moves. If a move is successful, current point is moved to the new location, otherwise on keeps the current point. Of course the successfulness of a local search technique depends on the neighbourhood chosen. In MAX-SAT, the input space will be the one given by all possible truth assignment and representing it as a binary string. The output will be the number of satisfied clauses. Formally, let $\Omega$ be the discrete search space: $\Omega = \{0,1\}^n$, and let $f:\Omega \rightarrow  {\rm I\!R}$ the function to be maximized. Then let $U^{(t)} \in \Omega$ be the current configuration along the search trajectory at step $t$ and $N(U^{(t)}) \subseteq \Omega$ the heighbourhood obtained applying a set of basic moves $\mu_i \ (1 \leq i \leq n)$ where $\mu_i$ complements the \textit{i}-th bit of the string: $\mu_i (u_1, u_2, ... , u_i, ... ,u_n) = (u_1, u_2, ..., \lnot u_i, ..., u_n)$.
$$
N(U^{(t)}) = \{U \in \Omega \text{ such that } U = \mu_i(U^{(t)}), \ i = 1,...,n\}
$$
The version of local search that we consider starts from a random initial configuration $U^{(0)} \in \Omega$ and generated a search trajectory as follows: 
\begin{itemize}
\item{1. $V \leftarrow \bestneighbor(N(U^{(t)}))$}
\item{2. $U^{(t+1)} = \begin{cases} 
      V & f(V) < f(U^{(t)}) \\
      U^{(t)} & f(V) \geq f(U^{(t)})
   \end{cases}
$}
\end{itemize}
Obviously, this algorithm stops when no other improving points can be reached.

An intriguing result by Koutsoupias and Papadimitriou shows that, for the vast majority of satisfiable 3-SAT formulae, the local search heuristic that start at a random truth assignments and repeatedly flips a variable that improves the number of satisfied clauses, almost always succeeds in discovering a satisfying truth assignment.

\subsubsection{Randomized search for 2-SAT (Markov process)}
While it is well known that 2-SAT is not NP-Complete, the MARKOV-SEARCH algorithm is interesting for its simplicity and its capacity to find a solution in $O(n^2)$ steps. it works as follow:
\begin{itemize}
\item{1. Start with a random truth assignment}
\item{2. While there are unsatisfied clause, pick one of them and flip a random literal in it}
\end{itemize}

Note that it accepts worsening moves.

\subsection{Memory-less Local Search Heuristics}
State-of-the-art heuristics for MAX-SAT are obtained by complementing local search with schemes that are capable of producing better approximations beyond the locally optimal points.
\subsubsection{SA for MAX-SAT}
The algorithm works like the normal SA, with MAX-TEMP and MIN-TEMP as bounds to the temperature. Figure \ref{fig:sasat} shows how it works.

\begin{figure}[H]
\includegraphics[scale=0.3]{sasat}
\caption{The SA for SAT}
\centering
\label{fig:sasat}
\end{figure}

\subsubsection{GSAT with ``random noise'' strategies}
SAT is of special concern to Artificial Intelligence because of its connection with reasoning. GSAT consists on multiple runs on LS, each run consisting in a number of iterations proportional to the input space dimension. GSAT-WITH-WALK use ``noise'' to escape from attraction basins, balancing random moves with deterministic ones. Picture \ref{fig:gsat} shows how it works:

\begin{figure}[H]
\includegraphics[scale=0.3]{gsat}
\caption{GSAT-WITH-WALK algorithm}
\centering
\label{fig:gsat}
\end{figure}


\subsection{History-sensitive Heuristics}

Different history-sensitive heuristics have been proposed to continue local search schemes beyond local optimality. These schemes aim at intensifying the search in promising regions and at diversifying the search into uncharted terri- tories by using the information collected from the previous phase (the history) of the search.

\subsubsection{Prohibition-based Search}
Prohibit some moves for a finite amount of time is a good practice to avoid local search fall in the same attraction basin many times and to avoid the creation of cycles. 

\subsection{Models of hardness and threshold effects}
How to benchmark our SAT solver or our MAX-SAT solver? There is the need to generate some test cases on which run our algorithms. There are two main models to generate MAX-SAT instances:
\begin{itemize}
\item{k-SAT modes, also called fixed length clause mode. A randomly generated CNF formula consists of independently generated random clauses, where each clause contains exactly $k$ literals. Each literal is chosen uniformly from $U = \{u_1 , ..., u_n \}$, and negated with probability $p$. The default value for $p$ is 1/2.}
\item{average k-SAT model, also called random clause model. A randomly generated CNF formula consists of independently generated random clauses. Each literal has ha probability $p$ of being part of a clause. In detail, each of the n variables occurs positively with probability $p(1 -p)$, negatively with probability $p(1 - p)$, both positively and negatively with probability $p^2$, and is absent with probability $(1 - p)^2.$}
\end{itemize}

\section{DOE and surrogate}
I need to study beahaviour of systems to find improving/optimal config.  Consumption of motors depends on various parameters: to minimize it, one needs a model of how the consumption depending on the design parameters.

I have a system transforming a vector of inputs X into an output Y.
Rarely exact analytic model is available, deriving the output Y requires running a simulator or even building prototypes, but these are too costly.
Minimizing the number of simulator runs or experiments is a therefore a critical issue. Building a surrogate (approximated) model based on the executed experiments can be a way, if the model evaluation is much faster than the real system. When an initial model is available, it can be used to make predictions or to identify optimal configurations by optimization.

If calculating $ Y(x) $ for different x values is very costly, all evaluations done are collected to build initial models, useful to guide the generation of the future input configurations to be evaluated.
This is a case of surrogate optimization, a case of learning while optimizing.

We focus on data-driven models for which experimental data is not available at the beginning but has to be acquired in a strategic and intelligent manner. 

We need experimental data to study a system or to generate training examples in machine learning, the objectives are to limit costs, while getting informative data, sufficient to build precise models.

In science and engineering it is known as design of experiments. In
ML, it is known as active or query learning.

\subsection{Design Of Experiments (DOE)}

Contrary to what is commonly believed, the experimental activity combines:
\begin{itemize}
\item creativity 
\item strategic focus on a goal to be reached 
\end{itemize}

If the focus is to model a system (identify improving configurations via automated optimization), sample input points need to be chosen carefully, depending on the goal and without wasting computational or real resources.

Inputs that do not influence the output in a significant manner can be eliminated from the model.

Solution: samples generated by DOE can be used as starting points for explorations by optimization techniques based on local search.

All experimental activity is usually accompanied by experimental noise in the form of measurement errors and by the possibility to be fooled by chance into deriving wrong conclusions.

Because physical measurements are subject to errors, one often assumes that the measured output $ Y(X) $ is equal to a “true” response $ Y_t(X) $ plus a random error term: $ Y(X) = Y_ t(X) + \epsilon $, $ \epsilon $  being a random variable chracterized by a given distribution (usually, a Gaussian with a given standard deviation $ \sigma $).

Choosing the appropriate design of experiments is not easy and the type of model and the problem have to be considered carefully.

In DOE, inputs are usually called “design variables” or “factors”, output is called “response”. Let’s note that no simple receipt exists to apply DOE and response surfaces. All techniques depend on assumptions that can be easily violated in real systems.

In many engineering applications, a simulator can be used to generate example outputs Y corresponding to user-chosen inputs X. The situation can be different if examples require running a physical system, or observing a process in action. 
But if one is in the lucky case, designing an experiment means deciding the appropriate number of examples to generate and where they are generated in the input space. 

A simple constraint on the input can be given by specifying a design space, usually by setting separate upper and lower bounds on each x variable, obtaining an hypercube. If one has reasons to believe that the form of the model is correct,placing the examples near or on the boundaries of the design space makes sense because this choice will minimize the effect of the random error $ \epsilon $ on the determination of the model parameters. Think about fitting a line with two points: the more separated they are in the x direction, the more stable will be the line with respect to random noise in the y position.

On the contrary, if the model is a rough approximation, and one
aims at using the model in the interior part of the design space, for example for finding an optimal configuration, a good fraction of the points should be placed also in the interior part of the design space.

Another critical parameter is the total number of examples. In general, the more examples the better but the computationalcost of running the simulator can be very large and one must settle for the minimal number which is sufficient to identify a workable model.

Finally, the number of examples has to be larger than the number of parameters in the model.

\subsubsection{full factorial design}
T his is the most obvious (and naive) way to proceed. It is based on determining the number of different values for each variables and then generating sample points on a regular grid.

The advantage of the full factorial design is its simplicity; its disadvantage lies in its regular grid-like nature. 
The number of points generated, if the number of levels \textit{L} is the same for all variables, is equal to $ L^n $, a number which will explode very rapidly to make this method applicable only to problems with a small number of input variables.

Full factorial designs are very expensive: the number of samples
grows exponentially when the input dimension grows. Reduced factorial designs using only a subset of the full factorial design points are considered.

\subsubsection{randomized design}
A more robust design, which can generate more points if more time is available, is based on a simple randomization. A randomized design with a uniform probab generates points in every area with a probability different from zero.

Pseudo-random number generators can be used to generate random numbers uniformly distributed in an interval. By repeated calls of a good-quality pseudo-random number generator, one can create random
vectors uniformly distributed in the design space.

It works for any number of samples, and that it can be immediately repeated (by using a different random number seed and therefore a different sequence of random numbers) to check for the robustness or fragility of the first DOE.

A weakness of the basic pseudo-random design is that, because of the random and independent nature of the samples, it will often leave large regions of the design space unexplored. The stratified Monte Carlo sampling was developed to cure the above problem, provided that one can afford the required number of samples. The idea is to divide each interval into subintervals of equal probability.

Once the bins are defined, a sample position is then randomly selected
within each bin. This trivially guarantees that at least a sample will be present in each interval.

\subsubsection{latin hypercube sampling}
Is a modern and popular randomized DOE method than can work with any user-selected number of samples k. A square grid containing sample positions is a Latin square if (and only if) there is only one sample in each row and each column. A Latin hypercube is the generalisation of this concept to an arbitrary number of dimensions, whereby each sample is the only one in each axis-aligned hyperplane containing it.

\subsection{surrogate model-based opt}

A surrogate model is an engineering method used when an outcome of interest cannot be easily directly measured. For many real-world problems, however, a single simulation can take many minutes, hours, or even days to complete. To alleviate this load on can use approximated models, known as surrogate models.

The surrogate model can be called “approximation model” or “response surface approximation”, leading to the term of Response
Surface Methodology (RSM) in optimization. The main goal of response surface methodology is to create a predictive model of relationship between the inputs and the outputs and then using the model to determine optimal operating settings for the system.

Surrogate models are constructed by using a data-driven, bottom-up
approach. The exact, inner working of the simulation code is not assumed to be known, solely the input-output behavior is available (the system is considered as a black box).

The surrogate models need to be trained by considering a set of example pairs (X, Y), giving input and the corresponding output for a set of example values, used to fix the model parameters.
if the models of the relationship between input and output is
given by a polynomial function with parameters, the model parameters can be fixed by requiring that some measure of the average difference, on the examples, between the Y values of the polynomial and the corresponding Y given values of the examples, is minimized.

An alternative when the functional form is not known is to consider non-parametric models in machine learning.

\subsection{active or query learning}
Acquiring training examples for supervised learning tasks is expensive.
Active learning approaches attempt to reduce this cost by actively suggesting inputs for which supervision should be collected, in an iterative process alternating learning and feedback.

The uncertainty sampling (US) principle considers the input with highest predictive uncertainty as the most informative training example for the current model.

The query-by-committee strategy maintains a committee of models, trained on the current set of examples and representing competing hypotheses. Each committee member votes on the output value of the candidate inputs, and the input considered most informative is the one on which they disagree most. 

The expected model change principle considers as the most informative query the input that, when added to the training set would yield the greatest change in the current model.

The expected error reduction criterion selects the input that is expected to yield the greatest reduction in the generalization error of the current model. Computing the expected generalization error is computationally expensive.

To overcome this limitation, the variance reduction approach queries the input that minimizes the model variance.

\section{Measuring problem difficulty in local search}
\subsection{Measuring and modeling problem difficulty}

When using machine learning strategies in the area of heuristics, finding appropriate features to measure and appro-
priate metrics is a precious guide for the design of effective methods and for explaining and understanding.

One aims at discovering relationships between problem characteristics and problem difficulty. Because the focus is on local search methods, one would like to characterize statistical properties of the solution landscape causing poor or slow results by local search. The effectiveness of a stochastic local search method is determined by how microscopic local decisions made at each search step interact to determine the macroscopic global behavior of the system. In particular, one studies how the function value f depends on the input configuration and changes after small and local modifications.

\subsection{phase transition in combinatorial problems}
Phase-transitions have been identified as a mechanism to study and explain problem difficulty. A phase transition in a physical system is characterized by the abrupt change of its macroscopic properties at certain values of the defining parameters. For example, consider the transitions from ice to water to steam at specific values of temperature and pressure.

A surprising result is that hard problem instances are concentrated near the same parameter values for a wide variety of common search
heuristics. This location also corresponds to a transition between solvable and unsolvable instances. For example, when the control parameter that is changed is the number of clauses in SAT instances, different schemes like complete backtracking and local search show very long computing times in the same transition region.

Udentifying zones where the most difficult problems are is relevant
for generating difficult instances to challenge algorithms. 
But it is not so easy to identify difficult instances of NP-hard problems. Computational complexity classes are defined through a worst-case analysis: in practice the worst cases may be very difficult to encounter or to generate.

\subsection{empirical models of fitness surfaces}
Empirical models of fitness surfaces can explain if local search will be effective and can help in selecting the proper local moves.

The performance of search algorithms depends on the features of the search space. In particular, a useful measure of variability of the search landscape is given by the correlation between the values of the objective function f over all pairs of configurations at distance d. The proper distance to be used depends on the nature of the solving technique. In local search the distance between two configurations X and X 0 can be measured measured as the minimum number of local steps to transform one into the other. 

Landscape Correlation Function:

\begin{figure}[H]
\includegraphics[scale=0.60]{landscape}
\centering
\end{figure}

This measure captures the idea of ruggedness of a surface: a low correlation function implies high statistical independence between points at distance d. While it is expectable that for large values of d the correlation R(d) goes to zero (unless the search landscape is very smooth), the value of R(1) can be meaningful.

$ R(1) \approx 0 $ means that, on average, there is little correlation
between the objective value at a given configuration and the value of its neighbors. This can signal of a poor choice of the neighborhood structure, or that the problem is particularly hard for local search. On the other hand, $ R(1)  \approx 1 $ is a clear indication that the neighborhood structure produces a smooth fitness surface.

Computing Landscape Correlation Function for large search spaces can be difficult. A common estimation technique uses random walks.

\begin{figure}[H]
\includegraphics[scale=0.60]{randomw}
\centering
\caption{Estimating autocorrelation on a rugged (top) and a smooth (bottom) landscape by means of a random walk.}
\end{figure}

\subsection{tunable landscapes}
Tunable landscapes are models used to generate problem instances. They allow to measure how variables interact with each others to the overall fitness. They are useful to validate different choices in algorithm design.
 
\subsection{Measuring local search components: diversification and bias}
Diversification-bias plots (D-B plots) can measure the effectiveness of single local search component and deliver insight on their balance of exploration versus exploitation. 

To ensure progress in algorithmic research it is not sufficient to have a horse-race of different algorithms on a set of instances and declare winners and losers. Actually, very little information can be obtained by these kinds of comparisons.

A better method is to design a generator of random instances so that it can produce instances used during the development and tuning phase, while a different set of instances extracted from the same generator is used for the final test, but still it does not explain why a method is better than another one.

The basic compromise to be reached is that between diversification and bias.
Random search is optimal for diversification but not for bias. Diversification can be associated with different metrics. Here we adopt the Hamming distance.

\begin{itemize}
\item diversification is measured with some distance metric (hamming). After selecting the metric, the diversification of simple random walk is analyzed to provide a basic system against which more complex components are evaluated. 
\item The diversification-bias plots (D-B plots) of different basic components are investigated and a conjecture is formulated that the best components for a given problem are the maximal elements in the diversification-bias (D-B) plane for a suitable partial ordering
\item The conjecture is validated by a competitive analysis of the components on a benchmark.
\end{itemize}

\subsubsection{better algorithms are Pareto-optimal in D-B plots}
A relation of partial order, denoted by the symbol $ \geq $ and called “domination,” is introduced in a set of algorithms in the following way: given two component algorithms A and B, A dominates B ($ A \geq B $) if and only if it has a larger or equal diversification and bias (the average f value).

For comparable results, the simplest algorithms (with less meta-parameters) should always be chosen because they are more robust and simpler to study and understand.


\end{document}




